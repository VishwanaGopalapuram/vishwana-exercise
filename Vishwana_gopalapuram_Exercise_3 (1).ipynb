{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Let's look at an interesting task for classifying text: figuring out how people feel about social media comments. The objective is to figure out whether a certain word shows a positive, negative, or neutral attitude. In order to find out how the public feels about their goods, services, or events, this activity is useful for companies and groups.\n",
        "\n",
        "#What follows are five types of features that could be useful for creating a machine learning model for mood analysis:\n",
        "\n",
        "#Features of Word Frequency:\n",
        "\n",
        "#By looking at how often words appear in a message, you can get a sense of how people feel about it.\n",
        "#Feature Example: Word Count for Positive and Negative Remarks.\n",
        "#\"Love,\" \"great,\" and \"amazing\" are examples of positive sentiments, whereas \"hate,\" \"disappointing,\" and \"terrible\" are examples of negative sentiments.\n",
        "#Letters called \"n-grams\"\n",
        "\n",
        "#Taking into account groups of words that are next to each other (n-grams) can help you understand the situation.\n",
        "#For example, bi-grams or tri-grams of words that are often put together.\n",
        "#Definition: Phrases like \"not good\" or \"highly recommend\" can add depth to meaning that a single word might miss.\n",
        "#Analysis of Emoticons and Emojis:\n",
        "\n",
        "#Description: Emoticons and emojis can show how you feel.\n",
        "#Type and number of emoticons or emojis used in the message is an example of a feature.\n",
        "#For example, ðŸ˜ˆ could mean a good feeling, while ðŸ˜¡ could mean a negative feeling.\n",
        "#Lexicons for Sentiment:\n",
        "\n",
        "#Lists of words that have already been described and their sentiment scores.\n",
        "#Example Feature: Sum of sentiment ratings for words in the comment calculated using a sentiment lexicon.\n",
        "#To explain: By giving words scores, you can record how strongly someone feels about something, and the comment's general sentiment can be seen as a whole.\n",
        "#Tags for Part-of-Speech (POS):\n",
        "\n",
        "#Figuring out what kind of grammar each word in a comment belongs to.\n",
        "#Number of adjectives or adverbs used in the message is an example of a feature.\n",
        "#Meaning: Adjectives and adverbs often show how someone feels. One way to differentiate between \"amazing product\" and \"slow service\" is by looking at the words actually used.\n",
        "#By putting these features together, a machine learning model can read different parts of text, which helps it understand and sort the feelings people have when they leave notes on social media sites. Accurately classifying mood requires preprocessing the text data, feature engineering, and picking the right algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f873eb47-f02c-4d70-f525-64700abb3a1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.10.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "\n",
            "Comment: I love this product! It's amazing.\n",
            "Word Frequency Features - Positive Count: 1, Negative Count: 0\n",
            "N-grams Features: [('i', 'love'), ('love', 'this'), ('this', 'product'), ('product', '!'), ('!', 'it'), ('it', \"'s\"), (\"'s\", 'amazing'), ('amazing', '.')]\n",
            "Emoji Features: []\n",
            "Sentiment Lexicon Features: 0.6125\n",
            "POS Tag Features - Adjectives and Adverbs Count: 1\n",
            "\n",
            "Comment: The service was terrible. I won't recommend it.\n",
            "Word Frequency Features - Positive Count: 0, Negative Count: 1\n",
            "N-grams Features: [('the', 'service'), ('service', 'was'), ('was', 'terrible'), ('terrible', '.'), ('.', 'i'), ('i', 'wo'), ('wo', \"n't\"), (\"n't\", 'recommend'), ('recommend', 'it'), ('it', '.')]\n",
            "Emoji Features: []\n",
            "Sentiment Lexicon Features: -1.0\n",
            "POS Tag Features - Adjectives and Adverbs Count: 2\n",
            "\n",
            "Comment: Not a bad experience, but could be better.\n",
            "Word Frequency Features - Positive Count: 0, Negative Count: 0\n",
            "N-grams Features: [('not', 'a'), ('a', 'bad'), ('bad', 'experience'), ('experience', ','), (',', 'but'), ('but', 'could'), ('could', 'be'), ('be', 'better'), ('better', '.')]\n",
            "Emoji Features: []\n",
            "Sentiment Lexicon Features: 0.42499999999999993\n",
            "POS Tag Features - Adjectives and Adverbs Count: 2\n",
            "\n",
            "Comment: ðŸ˜Š Great event! Enjoyed every moment.\n",
            "Word Frequency Features - Positive Count: 1, Negative Count: 0\n",
            "N-grams Features: [('ðŸ˜Š', 'great'), ('great', 'event'), ('event', '!'), ('!', 'enjoyed'), ('enjoyed', 'every'), ('every', 'moment'), ('moment', '.')]\n",
            "Emoji Features: [':smiling_face_with_smiling_eyes:']\n",
            "Sentiment Lexicon Features: 0.75\n",
            "POS Tag Features - Adjectives and Adverbs Count: 1\n",
            "\n",
            "Comment: Disappointed with the quality. ðŸ˜¡\n",
            "Word Frequency Features - Positive Count: 0, Negative Count: 0\n",
            "N-grams Features: [('disappointed', 'with'), ('with', 'the'), ('the', 'quality'), ('quality', '.'), ('.', 'ðŸ˜¡')]\n",
            "Emoji Features: [':enraged_face:']\n",
            "Sentiment Lexicon Features: -0.75\n",
            "POS Tag Features - Adjectives and Adverbs Count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk emoji textblob\n",
        "\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "import emoji\n",
        "from textblob import TextBlob\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text data\n",
        "sample_comments = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\",\n",
        "]\n",
        "\n",
        "def analyze_word_frequencies(input_text):\n",
        "    words = word_tokenize(input_text.lower())\n",
        "    positive_words = [\"love\", \"great\", \"awesome\"]\n",
        "    negative_words = [\"hate\", \"disappointing\", \"terrible\"]\n",
        "\n",
        "    positive_count = sum(1 for word in words if word in positive_words)\n",
        "    negative_count = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "    return positive_count, negative_count\n",
        "\n",
        "def generate_n_grams(input_text, n=2):\n",
        "    words = word_tokenize(input_text.lower())\n",
        "    n_grams_result = list(ngrams(words, n))\n",
        "    return n_grams_result\n",
        "\n",
        "def identify_emojis_modified(input_text):\n",
        "    text_with_emojis = emoji.demojize(input_text)\n",
        "    emojis = re.findall(r':[a-z_]+:', text_with_emojis)\n",
        "    return emojis\n",
        "\n",
        "def analyze_sentiment(input_text):\n",
        "    blob = TextBlob(input_text)\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "    return sentiment_score\n",
        "\n",
        "def count_pos_tags_modified(input_text, pos_tags=[\"JJ\", \"RB\"]):\n",
        "    words = word_tokenize(input_text)\n",
        "    pos_tags_count = sum(1 for _, tag in pos_tag(words) if tag in pos_tags)\n",
        "    return pos_tags_count\n",
        "\n",
        "# Feature extraction for each comment\n",
        "for sample_comment in sample_comments:\n",
        "    print(f\"\\nComment: {sample_comment}\")\n",
        "\n",
        "    positive_count, negative_count = analyze_word_frequencies(sample_comment)\n",
        "    print(f\"Word Frequency Features - Positive Count: {positive_count}, Negative Count: {negative_count}\")\n",
        "\n",
        "    n_grams_result = generate_n_grams(sample_comment, n=2)\n",
        "    print(f\"N-grams Features: {n_grams_result}\")\n",
        "\n",
        "    emojis_result = identify_emojis_modified(sample_comment)\n",
        "    print(f\"Emoji Features: {emojis_result}\")\n",
        "\n",
        "    sentiment_score_result = analyze_sentiment(sample_comment)\n",
        "    print(f\"Sentiment Lexicon Features: {sentiment_score_result}\")\n",
        "\n",
        "    pos_tags_count_result = count_pos_tags_modified(sample_comment, pos_tags=[\"JJ\", \"RB\"])\n",
        "    print(f\"POS Tag Features - Adjectives and Adverbs Count: {pos_tags_count_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "import pandas as pd\n",
        "\n",
        "# Sample features and labels\n",
        "input_features = [\n",
        "    [1, 0, [('i', 'love'), ('love', 'this')], 2, 0.6125, 3],\n",
        "    [0, 1, [('service', 'terrible'), ('recommend', 'it.')], 1, -0.5, 2],\n",
        "    [0, 0, [('bad', 'experience'), ('could', 'better.')], 0, 0.25, 1],\n",
        "    [1, 0, [('great', 'event'), ('enjoyed', 'moment.')], 1, 0.75, 2],\n",
        "    [0, 1, [('disappointed', 'quality.'), ('angry', 'face.')], 1, -0.8, 1],\n",
        "]\n",
        "\n",
        "labels = ['Positive', 'Negative', 'Neutral', 'Positive', 'Negative']\n",
        "\n",
        "# Create a DataFrame for the features\n",
        "original_df = pd.DataFrame(input_features, columns=['Positive_Count', 'Negative_Count', 'N-grams', 'Emoji_Count', 'Sentiment_Lexicon', 'Pos_Tags_Count'])\n",
        "\n",
        "# Extract labels\n",
        "original_y = [1 if label == 'Positive' else 0 for label in labels]\n",
        "\n",
        "# Preprocess 'N-grams' feature\n",
        "original_df['N-grams'] = original_df['N-grams'].apply(lambda x: len(x))  # For simplicity, using the count of n-grams as a feature\n",
        "\n",
        "# Make all features non-negative\n",
        "normalized_df = original_df - original_df.min().min()\n",
        "\n",
        "# Select the top k features\n",
        "k_value = 3  # You can adjust this value based on your requirements\n",
        "feature_selector = SelectKBest(chi2, k=k_value)\n",
        "selected_features_array = feature_selector.fit_transform(normalized_df, original_y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices = feature_selector.get_support(indices=True)\n",
        "\n",
        "# Display the selected features and their scores\n",
        "selected_feature_names = normalized_df.columns[selected_feature_indices]\n",
        "feature_scores_result = feature_selector.scores_[selected_feature_indices]\n",
        "sorted_selected_features = sorted(zip(selected_feature_names, feature_scores_result), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"Selected Features (Top {k_value}):\")\n",
        "for feature_name, score_value in sorted_selected_features:\n",
        "    print(f\"{feature_name}: {score_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOPRSI6GweAK",
        "outputId": "97250e3b-bcf9-4ca7-b405-2d6a81efad32"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features (Top 3):\n",
            "Sentiment_Lexicon: 1.4796195652173916\n",
            "Positive_Count: 0.9999999999999992\n",
            "Pos_Tags_Count: 0.628205128205128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f134d9-d66a-46dd-ffb2-12c0737af6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Ranked Documents based on Similarity:\n",
            "                                          Document  Similarity\n",
            "0               I love this product! It's amazing.    0.674666\n",
            "4                 Disappointed with the quality. ðŸ˜¡    0.614161\n",
            "3             ðŸ˜Š Great event! Enjoyed every moment.    0.595608\n",
            "2       Not a bad experience, but could be better.    0.579785\n",
            "1  The service was terrible. I won't recommend it.    0.574029\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers scikit-learn\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Sample text data\n",
        "text_data_samples = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\",\n",
        "]\n",
        "\n",
        "# Query\n",
        "search_query = \"Looking for a great product with excellent service.\"\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode the query\n",
        "query_tokens_encoded = tokenizer_bert(search_query, return_tensors='pt', padding=True, truncation=True)\n",
        "query_output_embeddings = bert_model(**query_tokens_encoded)\n",
        "\n",
        "# Extract the embeddings for the query\n",
        "query_embedding_result = query_output_embeddings.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Tokenize and encode each document in the dataset\n",
        "document_embeddings_result = []\n",
        "for document_text in text_data_samples:\n",
        "    document_tokens_encoded = tokenizer_bert(document_text, return_tensors='pt', padding=True, truncation=True)\n",
        "    document_output_embeddings = bert_model(**document_tokens_encoded)\n",
        "    document_embedding_result = document_output_embeddings.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    document_embeddings_result.append(document_embedding_result)\n",
        "\n",
        "# Calculate cosine similarity between the query and each document\n",
        "similarity_scores_result = [cosine_similarity(query_embedding_result, doc_embedding)[0][0] for doc_embedding in document_embeddings_result]\n",
        "\n",
        "# Create a DataFrame to display results\n",
        "result_df_updated = pd.DataFrame({'Document': text_data_samples, 'Similarity': similarity_scores_result})\n",
        "\n",
        "# Sort documents based on similarity in descending order\n",
        "result_df_updated = result_df_updated.sort_values(by='Similarity', ascending=False)\n",
        "\n",
        "# Display the ranked documents\n",
        "print(\"Ranked Documents based on Similarity:\")\n",
        "print(result_df_updated)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Working on extracting features from text data is a very important way to learn how to describe textual data for machine learning tasks. Tokenization, feature extraction, and the use of external libraries such as NLTK and scikit-learn were helpful in becoming familiar with the process of preprocessing and transforming text input into a format that machine learning models could handle. It was clear how important it was to choose the right features and know how they affected the model's performance.\n",
        "\n",
        "#Problems: One problem was making sure that different libraries could work together and meeting the specific needs of each library. For example, working with the NLTK punkt resource and changing the character feature extraction because the library had changed showed how important it is to know a lot about the library. It was also hard to pull features from text data that had emojis, special characters, and different languages.\n",
        "\n",
        "#Important for your field of study:\n",
        "#As a component of Natural Language Processing (NLP), the exercise has a direct connection to activities that involve the analysis of text data and machine learning. Essential to NLP, feature extraction decides how written data is stored for models. Word frequencies, n-grams, sentiment scores, and part-of-speech tags are used in NLP for sentiment analysis, text classification, and information retrieval. The fact that BERT is used to rank text similarity shows how important advanced NLP methods are in real life settings. Overall, the activity teaches us useful things about preprocessing and feature building for NLP tasks.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
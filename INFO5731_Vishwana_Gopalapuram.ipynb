{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247431a8-a9e8-4da5-88b6-fe3f2af49b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Reviews\n",
            "0          The quality of the item is great\\nRead more\n",
            "1                     Just what I expected.\\nRead more\n",
            "2    The media could not be loaded.\\n              ...\n",
            "3    I've had this watch for 3 days now. I got it f...\n",
            "4    The media could not be loaded.\\n              ...\n",
            "..                                                 ...\n",
            "795  I've had this watch for 3 days now. I got it f...\n",
            "796  The media could not be loaded.\\n              ...\n",
            "797                       Amazing I love ❤️\\nRead more\n",
            "798  Honestly exceeded my expectations. What a beau...\n",
            "799  The watch is gaudy. I paid $94.00 for this pie...\n",
            "\n",
            "[800 rows x 1 columns]\n",
            "Reviews saved to reviews.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_reviews(source_url, num_reviews, output_csv):\n",
        "    all_reviews = []\n",
        "\n",
        "    # Amazon reviews have a fixed number of reviews per page (usually 10), so we can calculate the number of pages needed\n",
        "    pages_needed = (num_reviews // 10) + (1 if num_reviews % 10 != 0 else 0)\n",
        "\n",
        "    for page_number in range(1, pages_needed + 1):\n",
        "        # URL to scrape the data from\n",
        "        url = f\"{source_url}&pageNumber={page_number}\"\n",
        "\n",
        "        # Using the requests module to get the web page content\n",
        "        page = requests.get(url)\n",
        "\n",
        "        # Parsing the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "        # Finding all the review elements\n",
        "        review_elements = soup.find_all('div', class_='a-section review aok-relative')\n",
        "\n",
        "        for review in review_elements:\n",
        "            review_text = review.find('span', class_='review-text')\n",
        "            if review_text:\n",
        "                review_text = review_text.get_text().strip()\n",
        "                all_reviews.append(review_text)\n",
        "\n",
        "                # Break the loop if the desired number of reviews is collected\n",
        "                if len(all_reviews) == num_reviews:\n",
        "                    break\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame({'Reviews': all_reviews})\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL of the product on Amazon\n",
        "    amazon_product_url = \"https://www.amazon.com/Fossil-Womens-Stella-Stainless-Chronograph/dp/B00KGTUKFU/ref=sr_1_7?crid=WEP1EJ0NCYIB&dib=eyJ2IjoiMSJ9.5NV6CyBgRX7xVrm3oZfd1dIxSlkLWx2llT2CaPsXDMQ9fMiUzTARP0O1FPV38FnJx5wczAWZ0828FwWaq-_RMpgr4Rp9jVv4CqqkngfwTjebACKL186f9LVJ-FtLWFIydTp_vQc3hcqUWCusU_s4EV7wGQzlFOMDPKLaBr-JFd24TXlccLsLJayvokqArbUvEUfmGmP8WoOoGj-L7Zzrapojx5VsUBF0UysSKr8uEPtHXZMvT-e5Cfhwer4y7ZtkMXNeR4apneWRrbTH1eCc08i5nSBk-6ydNGZjFsIz5wg.IMpYYpboVFoWNMeR3o4aHfZl80ZwXd2JQoBy7a4h5B8&dib_tag=se&keywords=fossil%2Bwatch%2Bwomen&qid=1709164745&sprefix=fossil%2Caps%2C127&sr=8-7&th=1\"\n",
        "    # Number of reviews to scrape\n",
        "    num_of_reviews = 1000  # Adjust the number of reviews as needed\n",
        "    # Output CSV file name\n",
        "    output_csv_file = \"reviews.csv\"\n",
        "\n",
        "    reviews_df = scrape_reviews(amazon_product_url, num_of_reviews, output_csv_file)\n",
        "\n",
        "    # Print the DataFrame\n",
        "    print(reviews_df)\n",
        "    print(f\"Reviews saved to {output_csv_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc04a1d-5156-40bc-ce83-a93c15533216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "                                             Reviews\n",
            "0        The quality of the item is great\\nRead more\n",
            "1                   Just what I expected.\\nRead more\n",
            "2  The media could not be loaded.\\n              ...\n",
            "3  I've had this watch for 3 days now. I got it f...\n",
            "4  The media could not be loaded.\\n              ...\n",
            "\n",
            "After removing punctuation and special characters:\n",
            "                                             Reviews  \\\n",
            "0        The quality of the item is great\\nRead more   \n",
            "1                   Just what I expected.\\nRead more   \n",
            "2  The media could not be loaded.\\n              ...   \n",
            "3  I've had this watch for 3 days now. I got it f...   \n",
            "4  The media could not be loaded.\\n              ...   \n",
            "\n",
            "                                     cleaned_reviews  \n",
            "0        The quality of the item is great\\nRead more  \n",
            "1                    Just what I expected\\nRead more  \n",
            "2  The media could not be loaded\\n               ...  \n",
            "3  Ive had this watch for 3 days now I got it for...  \n",
            "4  The media could not be loaded\\n               ...  \n",
            "\n",
            "After removing numbers:\n",
            "                                             Reviews  \\\n",
            "0        The quality of the item is great\\nRead more   \n",
            "1                   Just what I expected.\\nRead more   \n",
            "2  The media could not be loaded.\\n              ...   \n",
            "3  I've had this watch for 3 days now. I got it f...   \n",
            "4  The media could not be loaded.\\n              ...   \n",
            "\n",
            "                                     cleaned_reviews  \n",
            "0        The quality of the item is great\\nRead more  \n",
            "1                    Just what I expected\\nRead more  \n",
            "2  The media could not be loaded\\n               ...  \n",
            "3  Ive had this watch for  days now I got it for ...  \n",
            "4  The media could not be loaded\\n               ...  \n",
            "\n",
            "After removing stopwords:\n",
            "                                             Reviews  \\\n",
            "0        The quality of the item is great\\nRead more   \n",
            "1                   Just what I expected.\\nRead more   \n",
            "2  The media could not be loaded.\\n              ...   \n",
            "3  I've had this watch for 3 days now. I got it f...   \n",
            "4  The media could not be loaded.\\n              ...   \n",
            "\n",
            "                                     cleaned_reviews  \n",
            "0                            quality item great Read  \n",
            "1                                      expected Read  \n",
            "2  media could loaded good quality works well gir...  \n",
            "3  Ive watch days got birthday far lively really ...  \n",
            "4  media could loaded encanto estoy feliz con las...  \n",
            "\n",
            "After converting text to lowercase:\n",
            "                                             Reviews  \\\n",
            "0        The quality of the item is great\\nRead more   \n",
            "1                   Just what I expected.\\nRead more   \n",
            "2  The media could not be loaded.\\n              ...   \n",
            "3  I've had this watch for 3 days now. I got it f...   \n",
            "4  The media could not be loaded.\\n              ...   \n",
            "\n",
            "                                     cleaned_reviews  \n",
            "0                            quality item great read  \n",
            "1                                      expected read  \n",
            "2  media could loaded good quality works well gir...  \n",
            "3  ive watch days got birthday far lively really ...  \n",
            "4  media could loaded encanto estoy feliz con las...  \n",
            "\n",
            "After applying stemming:\n",
            "                                             Reviews  \\\n",
            "0        The quality of the item is great\\nRead more   \n",
            "1                   Just what I expected.\\nRead more   \n",
            "2  The media could not be loaded.\\n              ...   \n",
            "3  I've had this watch for 3 days now. I got it f...   \n",
            "4  The media could not be loaded.\\n              ...   \n",
            "\n",
            "                                     cleaned_reviews  \n",
            "0                            qualiti item great read  \n",
            "1                                        expect read  \n",
            "2  media could load good qualiti work well girlfr...  \n",
            "3  ive watch day got birthday far live realli cut...  \n",
            "4  media could load encanto estoy feliz con la co...  \n",
            "\n",
            "After applying lemmatization:\n",
            "                                             Reviews  \\\n",
            "0        The quality of the item is great\\nRead more   \n",
            "1                   Just what I expected.\\nRead more   \n",
            "2  The media could not be loaded.\\n              ...   \n",
            "3  I've had this watch for 3 days now. I got it f...   \n",
            "4  The media could not be loaded.\\n              ...   \n",
            "\n",
            "                                     cleaned_reviews  \n",
            "0                            qualiti item great read  \n",
            "1                                        expect read  \n",
            "2  medium could load good qualiti work well girlf...  \n",
            "3  ive watch day got birthday far live realli cut...  \n",
            "4  medium could load encanto estoy feliz con la c...  \n",
            "Text data cleaned and saved in 'cleaned_reviews.csv'\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('reviews.csv')\n",
        "\n",
        "# Function to remove punctuation and special characters from text\n",
        "def clean_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Function to remove numbers from text\n",
        "def clean_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Function to remove stopwords from text\n",
        "def remove_stop_words(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
        "\n",
        "# Function to convert text to lowercase\n",
        "def convert_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Function to apply stemming to text\n",
        "def apply_stemming(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    return ' '.join(stemmer.stem(word) for word in text.split())\n",
        "\n",
        "# Function to apply lemmatization to text\n",
        "def apply_lemmatization(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Apply text preprocessing steps to the Amazon reviews\n",
        "    print(\"Original DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    df['cleaned_reviews'] = df['Reviews'].copy()\n",
        "    df['cleaned_reviews'] = df['cleaned_reviews'].apply(clean_punctuation)\n",
        "    print(\"\\nAfter removing punctuation and special characters:\")\n",
        "    print(df.head())\n",
        "\n",
        "    df['cleaned_reviews'] = df['cleaned_reviews'].apply(clean_numbers)\n",
        "    print(\"\\nAfter removing numbers:\")\n",
        "    print(df.head())\n",
        "\n",
        "    df['cleaned_reviews'] = df['cleaned_reviews'].apply(remove_stop_words)\n",
        "    print(\"\\nAfter removing stopwords:\")\n",
        "    print(df.head())\n",
        "\n",
        "    df['cleaned_reviews'] = df['cleaned_reviews'].apply(convert_lowercase)\n",
        "    print(\"\\nAfter converting text to lowercase:\")\n",
        "    print(df.head())\n",
        "\n",
        "    df['cleaned_reviews'] = df['cleaned_reviews'].apply(apply_stemming)\n",
        "    print(\"\\nAfter applying stemming:\")\n",
        "    print(df.head())\n",
        "\n",
        "    df['cleaned_reviews'] = df['cleaned_reviews'].apply(apply_lemmatization)\n",
        "    print(\"\\nAfter applying lemmatization:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Save the modified DataFrame to CSV file\n",
        "    df.to_csv('cleaned_reviews.csv', index=False)\n",
        "\n",
        "    # Print message indicating successful completion\n",
        "    print(\"Text data cleaned and saved in 'cleaned_reviews.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp_custom = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load the cleaned text data\n",
        "df_custom_text = pd.read_csv('cleaned_reviews.csv')\n",
        "\n",
        "# Function to perform POS tagging and count POS\n",
        "def pos_tagging_and_count_custom_text(text):\n",
        "    doc_custom = nlp_custom(text)\n",
        "    pos_tags_custom = [token.pos_ for token in doc_custom]\n",
        "    pos_count_custom = Counter(pos_tags_custom)\n",
        "    return pos_count_custom\n",
        "\n",
        "# Function to perform constituency parsing and display the tree\n",
        "def constituency_parsing_custom_text(text):\n",
        "    doc_custom = nlp_custom(text)\n",
        "    for sent_custom in doc_custom.sents:\n",
        "        displacy.render(sent_custom, style='dep', jupyter=True, options={'distance': 90})\n",
        "\n",
        "# Function to perform dependency parsing and display the tree\n",
        "def dependency_parsing_custom_text(text):\n",
        "    doc_custom = nlp_custom(text)\n",
        "    for sent_custom in doc_custom.sents:\n",
        "        displacy.render(sent_custom, style='dep', jupyter=True)\n",
        "\n",
        "# Function to perform Named Entity Recognition (NER) and count entities\n",
        "def named_entity_recognition_custom_text(text):\n",
        "    doc_custom = nlp_custom(text)\n",
        "    entities_custom = [ent_custom.text for ent_custom in doc_custom.ents]\n",
        "    entity_count_custom = Counter(entities_custom)\n",
        "    return entity_count_custom\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose one sentence for example analysis\n",
        "    example_sentence_custom_text = df_custom_text['cleaned_reviews'].iloc[0]\n",
        "\n",
        "    # (1) POS Tagging\n",
        "    pos_count_custom_text = pos_tagging_and_count_custom_text(example_sentence_custom_text)\n",
        "    print(\"\\n(1) Parts of Speech (POS) Tagging:\")\n",
        "    print(pos_count_custom_text)\n",
        "\n",
        "    # (2) Constituency Parsing\n",
        "    print(\"\\n(2) Constituency Parsing:\")\n",
        "    constituency_parsing_custom_text(example_sentence_custom_text)\n",
        "\n",
        "    # (2) Dependency Parsing\n",
        "    print(\"\\n(3) Dependency Parsing:\")\n",
        "    dependency_parsing_custom_text(example_sentence_custom_text)\n",
        "\n",
        "    # (3) Named Entity Recognition (NER)\n",
        "    entity_count_custom_text = named_entity_recognition_custom_text(example_sentence_custom_text)\n",
        "    print(\"\\n(4) Named Entity Recognition (NER):\")\n",
        "    print(entity_count_custom_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "8o9kAj-LVKCj",
        "outputId": "39126a09-c090-4ae3-99d0-913a2b5c3805"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(1) Parts of Speech (POS) Tagging:\n",
            "Counter({'ADJ': 2, 'NOUN': 2})\n",
            "\n",
            "(2) Constituency Parsing:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e3aa4e2074984b72986d3a3ab60a9072-0\" class=\"displacy\" width=\"410\" height=\"227.0\" direction=\"ltr\" style=\"max-width: none; height: 227.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">qualiti</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">item</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">great</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">read</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e3aa4e2074984b72986d3a3ab60a9072-0-0\" stroke-width=\"2px\" d=\"M70,92.0 C70,47.0 135.0,47.0 135.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e3aa4e2074984b72986d3a3ab60a9072-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,94.0 L62,82.0 78,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e3aa4e2074984b72986d3a3ab60a9072-0-1\" stroke-width=\"2px\" d=\"M250,92.0 C250,47.0 315.0,47.0 315.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e3aa4e2074984b72986d3a3ab60a9072-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M250,94.0 L242,82.0 258,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-e3aa4e2074984b72986d3a3ab60a9072-0-2\" stroke-width=\"2px\" d=\"M160,92.0 C160,2.0 320.0,2.0 320.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-e3aa4e2074984b72986d3a3ab60a9072-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M320.0,94.0 L328.0,82.0 312.0,82.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(3) Dependency Parsing:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6395006a376d4156a5e698299d677aaa-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">qualiti</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">item</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">great</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">read</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6395006a376d4156a5e698299d677aaa-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6395006a376d4156a5e698299d677aaa-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6395006a376d4156a5e698299d677aaa-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6395006a376d4156a5e698299d677aaa-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-6395006a376d4156a5e698299d677aaa-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-6395006a376d4156a5e698299d677aaa-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(4) Named Entity Recognition (NER):\n",
            "Counter({'qualiti': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This assignment is very challeging to me compared to the previous assignment and excercises\n",
        "# few of the topics are new to me like POS tagging, parsing\n",
        "#Understanding the Amazon website's structure and locating the HTML sections containing the reviews was the difficult aspect. The HTML structure of Amazon can be intricate, necessitating meticulous inspection to identify the elements for scraping.\n",
        "#The assignment provided a valuable opportunity to practice online scraping techniques for data collection and organization. The course offered a pragmatic insight into dealing with dynamic web sites and managing any obstacles encountered while scraping data.\n"
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XMa1Lm4NTK3o"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd40b634-945c-4cf0-c817-bc0f625d5032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best number of topics: 5 (Coherence Score: 0.5349)\n",
            "Topics:\n",
            "Topic 1: 0.125*\"bad\" + 0.125*\"experience\" + 0.125*\"could\" + 0.125*\"better\" + 0.125*\"quality\" + 0.124*\"disappointed\" + 0.021*\"love\" + 0.021*\"amazing\" + 0.021*\"moment\" + 0.021*\"product\"\n",
            "Topic 2: 0.056*\"disappointed\" + 0.056*\"quality\" + 0.056*\"product\" + 0.056*\"love\" + 0.056*\"moment\" + 0.056*\"could\" + 0.056*\"wo\" + 0.055*\"better\" + 0.055*\"recommend\" + 0.055*\"terrible\"\n",
            "Topic 3: 0.056*\"disappointed\" + 0.056*\"love\" + 0.056*\"better\" + 0.056*\"product\" + 0.056*\"amazing\" + 0.056*\"quality\" + 0.056*\"moment\" + 0.056*\"recommend\" + 0.056*\"could\" + 0.056*\"bad\"\n",
            "Topic 4: 0.181*\"amazing\" + 0.181*\"product\" + 0.181*\"love\" + 0.031*\"disappointed\" + 0.031*\"moment\" + 0.031*\"quality\" + 0.031*\"bad\" + 0.031*\"better\" + 0.031*\"wo\" + 0.031*\"could\"\n",
            "Topic 5: 0.095*\"great\" + 0.095*\"event\" + 0.095*\"enjoyed\" + 0.095*\"every\" + 0.095*\"terrible\" + 0.095*\"wo\" + 0.095*\"service\" + 0.095*\"recommend\" + 0.095*\"moment\" + 0.016*\"disappointed\"\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\"\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lowercasing and removing punctuation\n",
        "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_docs = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "dictionary = corpora.Dictionary(preprocessed_docs)\n",
        "\n",
        "\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
        "\n",
        "\n",
        "coherence_scores = []\n",
        "for k in range(2, 10):\n",
        "    lda_model = models.LdaModel(corpus, num_topics=k, id2word=dictionary)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append((k, coherence_lda))\n",
        "\n",
        "best_k, best_coherence = max(coherence_scores, key=lambda x: x[1])\n",
        "print(f\"Best number of topics: {best_k} (Coherence Score: {best_coherence:.4f})\")\n",
        "\n",
        "# Train the LDA model with the best number of topics\n",
        "best_lda_model = models.LdaModel(corpus, num_topics=best_k, id2word=dictionary)\n",
        "\n",
        "# Print the topics\n",
        "print(\"Topics:\")\n",
        "for idx, topic in best_lda_model.print_topics():\n",
        "    print(f\"Topic {idx + 1}: {topic}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3769ff-5804-4904-dfcd-e3cc7eca688a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best number of topics: 3 (Coherence Score: 0.5349)\n",
            "Topics:\n",
            "Topic 1: -0.447*\"enjoyed\" + -0.447*\"moment\" + -0.447*\"great\" + -0.447*\"event\" + -0.447*\"every\" + -0.000*\"disappointed\" + -0.000*\"quality\" + -0.000*\"amazing\" + 0.000*\"terrible\" + 0.000*\"could\"\n",
            "Topic 2: 0.474*\"experience\" + 0.474*\"bad\" + 0.474*\"could\" + 0.474*\"better\" + -0.158*\"wo\" + -0.158*\"terrible\" + -0.158*\"service\" + -0.158*\"recommend\" + 0.000*\"disappointed\" + 0.000*\"quality\"\n",
            "Topic 3: -0.474*\"recommend\" + -0.474*\"service\" + -0.474*\"terrible\" + -0.474*\"wo\" + -0.158*\"experience\" + -0.158*\"better\" + -0.158*\"bad\" + -0.158*\"could\" + 0.000*\"disappointed\" + 0.000*\"quality\"\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models.lsimodel import LsiModel\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\"\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lowercasing and removing punctuation\n",
        "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_docs = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "dictionary = corpora.Dictionary(preprocessed_docs)\n",
        "\n",
        "# Create a corpus: a list of bag-of-words representation of each document\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
        "\n",
        "\n",
        "coherence_scores = []\n",
        "for k in range(2, 10):\n",
        "    lsi_model = LsiModel(corpus, num_topics=k, id2word=dictionary)\n",
        "    coherence_model_lsi = CoherenceModel(model=lsi_model, texts=preprocessed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_lsi = coherence_model_lsi.get_coherence()\n",
        "    coherence_scores.append((k, coherence_lsi))\n",
        "\n",
        "# Select the number of topics with the highest coherence score\n",
        "best_k, best_coherence = max(coherence_scores, key=lambda x: x[1])\n",
        "print(f\"Best number of topics: {best_k} (Coherence Score: {best_coherence:.4f})\")\n",
        "\n",
        "# Train the LSA model with the best number of topics\n",
        "best_lsi_model = LsiModel(corpus, num_topics=best_k, id2word=dictionary)\n",
        "\n",
        "# Print the topics\n",
        "print(\"Topics:\")\n",
        "for idx, topic in best_lsi_model.print_topics():\n",
        "    print(f\"Topic {idx + 1}: {topic}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\"\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.Defaults.stop_words |= {\"!\", \",\", \".\", \"'\", \"?\", \"(\", \")\", \":\", \";\", \"-\", \"ðŸ˜Š\", \"ðŸ˜¡\"}\n",
        "stop_words = set(stopwords.words('english'))\n",
        "preprocessed_docs = []\n",
        "for doc in documents:\n",
        "    tokens = nlp(doc)\n",
        "    tokens = [token.text.lower() for token in tokens if token.text.lower() not in stop_words]\n",
        "    preprocessed_docs.append(tokens)\n",
        "\n",
        "dictionary = corpora.Dictionary(preprocessed_docs)\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
        "\n",
        "# Calculate coherence score for different numbers of topics\n",
        "coherence_scores = []\n",
        "for num_topics in range(2, 11):\n",
        "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append((num_topics, coherence_score))\n",
        "\n",
        "# Print coherence scores\n",
        "print(\"Coherence Scores:\")\n",
        "for num_topics, score in coherence_scores:\n",
        "    print(f\"Number of Topics: {num_topics}, Coherence Score: {score}\")\n",
        "\n",
        "# Find the optimal number of topics based on coherence score\n",
        "optimal_num_topics, optimal_coherence_score = max(coherence_scores, key=lambda x: x[1])\n",
        "print(f\"\\nOptimal Number of Topics: {optimal_num_topics}, Coherence Score: {optimal_coherence_score}\")\n",
        "\n",
        "# Summarize topics for the optimal number of topics\n",
        "lda_model = models.LdaModel(corpus, num_topics=optimal_num_topics, id2word=dictionary, passes=15)\n",
        "print(\"\\nTopics:\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic {idx}: {topic}\")\n",
        "\n",
        "# Visualize the topics\n",
        "lda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4VNH99ZGoXqy",
        "outputId": "facd34d9-0ba4-4b37-ce7e-3c9074f40197"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Scores:\n",
            "Number of Topics: 2, Coherence Score: 0.5008437441772385\n",
            "Number of Topics: 3, Coherence Score: 0.493111586267488\n",
            "Number of Topics: 4, Coherence Score: 0.5208188946378115\n",
            "Number of Topics: 5, Coherence Score: 0.5214631592915472\n",
            "Number of Topics: 6, Coherence Score: 0.508575902086989\n",
            "Number of Topics: 7, Coherence Score: 0.5212548976531373\n",
            "Number of Topics: 8, Coherence Score: 0.5264444838426028\n",
            "Number of Topics: 9, Coherence Score: 0.5320430369597641\n",
            "Number of Topics: 10, Coherence Score: 0.5302354902413622\n",
            "\n",
            "Optimal Number of Topics: 9, Coherence Score: 0.5320430369597641\n",
            "\n",
            "Topics:\n",
            "Topic 0: 0.042*\"quality\" + 0.042*\"!\" + 0.042*\"disappointed\" + 0.042*\"service\" + 0.042*\"event\" + 0.042*\"ðŸ˜¡\" + 0.042*\"n't\" + 0.042*\"love\" + 0.042*\"better\" + 0.042*\"experience\"\n",
            "Topic 1: 0.196*\"disappointed\" + 0.196*\"ðŸ˜¡\" + 0.196*\"quality\" + 0.020*\"!\" + 0.020*\",\" + 0.020*\"could\" + 0.020*\"love\" + 0.020*\"bad\" + 0.020*\"terrible\" + 0.020*\"service\"\n",
            "Topic 2: 0.042*\"!\" + 0.042*\"quality\" + 0.042*\"service\" + 0.042*\"ðŸ˜¡\" + 0.042*\"bad\" + 0.042*\"love\" + 0.042*\"disappointed\" + 0.042*\"better\" + 0.042*\"amazing\" + 0.042*\"n't\"\n",
            "Topic 3: 0.145*\"experience\" + 0.145*\"better\" + 0.145*\",\" + 0.145*\"bad\" + 0.145*\"could\" + 0.014*\"!\" + 0.014*\"disappointed\" + 0.014*\"service\" + 0.014*\"ðŸ˜¡\" + 0.014*\"quality\"\n",
            "Topic 4: 0.115*\"enjoyed\" + 0.115*\"ðŸ˜Š\" + 0.115*\"moment\" + 0.115*\"great\" + 0.115*\"every\" + 0.115*\"event\" + 0.115*\"!\" + 0.011*\"ðŸ˜¡\" + 0.011*\"quality\" + 0.011*\"service\"\n",
            "Topic 5: 0.042*\"quality\" + 0.042*\"disappointed\" + 0.042*\"service\" + 0.042*\"!\" + 0.042*\"could\" + 0.042*\",\" + 0.042*\"terrible\" + 0.042*\"bad\" + 0.042*\"love\" + 0.042*\"n't\"\n",
            "Topic 6: 0.042*\"!\" + 0.042*\"disappointed\" + 0.042*\"quality\" + 0.042*\"bad\" + 0.042*\"ðŸ˜¡\" + 0.042*\"terrible\" + 0.042*\"event\" + 0.042*\"could\" + 0.042*\"service\" + 0.042*\"recommend\"\n",
            "Topic 7: 0.145*\"wo\" + 0.145*\"n't\" + 0.145*\"recommend\" + 0.145*\"service\" + 0.145*\"terrible\" + 0.014*\"quality\" + 0.014*\"ðŸ˜¡\" + 0.014*\"!\" + 0.014*\"disappointed\" + 0.014*\"bad\"\n",
            "Topic 8: 0.145*\"!\" + 0.145*\"amazing\" + 0.145*\"product\" + 0.145*\"'s\" + 0.145*\"love\" + 0.014*\"quality\" + 0.014*\"terrible\" + 0.014*\"disappointed\" + 0.014*\"ðŸ˜¡\" + 0.014*\",\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el58951386353809373767072921303\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el58951386353809373767072921303_data = {\"mdsDat\": {\"x\": [-0.0018173797681367955, -0.030523064286759615, -0.0018173790128984926, -0.11003961648334688, 0.1531638939760648, -0.0018173851120459727, -0.0018173798952677053, -0.11003961883365154, 0.10470792941604203], \"y\": [1.2644637544714606e-09, 1.909729535940956e-09, 2.478172999531859e-10, -0.1548411776598198, 3.553839864340922e-09, -7.688290717259933e-10, -7.688288748372858e-10, 0.1548411755770463, -3.355418976008261e-09], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [1.8333362130573356, 10.8333331481864, 1.8333362130573356, 18.499997165128494, 26.333317588616882, 1.8333362130573356, 1.8333362130573356, 18.499997165128494, 18.50001008071039]}, \"tinfo\": {\"Term\": [\"disappointed\", \"\\ud83d\\ude21\", \"quality\", \"!\", \"n't\", \"service\", \"recommend\", \"better\", \"experience\", \"wo\", \"bad\", \"could\", \"terrible\", \",\", \"product\", \"'s\", \"amazing\", \"love\", \"event\", \"every\", \"\\ud83d\\ude0a\", \"moment\", \"enjoyed\", \"great\", \"quality\", \"disappointed\", \"\\ud83d\\ude21\", \"service\", \"n't\", \"bad\", \",\", \"could\", \"terrible\", \"experience\", \"better\", \"recommend\", \"wo\", \"love\", \"'s\", \"amazing\", \"product\", \"event\", \"every\", \"\\ud83d\\ude0a\", \"moment\", \"enjoyed\", \"great\", \"!\", \"disappointed\", \"\\ud83d\\ude21\", \"quality\", \"bad\", \",\", \"could\", \"terrible\", \"service\", \"wo\", \"better\", \"n't\", \"recommend\", \"experience\", \"love\", \"amazing\", \"'s\", \"product\", \"event\", \"great\", \"\\ud83d\\ude0a\", \"moment\", \"enjoyed\", \"every\", \"!\", \"quality\", \"\\ud83d\\ude21\", \"disappointed\", \"service\", \"bad\", \",\", \"could\", \"terrible\", \"wo\", \"better\", \"n't\", \"experience\", \"love\", \"recommend\", \"amazing\", \"product\", \"'s\", \"event\", \"every\", \"\\ud83d\\ude0a\", \"moment\", \"enjoyed\", \"great\", \"!\", \"experience\", \"better\", \"bad\", \",\", \"could\", \"disappointed\", \"quality\", \"\\ud83d\\ude21\", \"service\", \"terrible\", \"n't\", \"recommend\", \"wo\", \"love\", \"product\", \"amazing\", \"'s\", \"event\", \"great\", \"\\ud83d\\ude0a\", \"moment\", \"enjoyed\", \"every\", \"!\", \"every\", \"\\ud83d\\ude0a\", \"moment\", \"enjoyed\", \"great\", \"event\", \"!\", \"\\ud83d\\ude21\", \"quality\", \"disappointed\", \"service\", \",\", \"could\", \"terrible\", \"recommend\", \"bad\", \"n't\", \"experience\", \"wo\", \"better\", \"love\", \"product\", \"'s\", \"amazing\", \"disappointed\", \"quality\", \"\\ud83d\\ude21\", \"could\", \"service\", \"bad\", \",\", \"terrible\", \"n't\", \"experience\", \"recommend\", \"wo\", \"better\", \"love\", \"product\", \"amazing\", \"'s\", \"event\", \"every\", \"\\ud83d\\ude0a\", \"moment\", \"enjoyed\", \"great\", \"!\", \"disappointed\", \"quality\", \"\\ud83d\\ude21\", \"bad\", \"could\", \"terrible\", \"service\", \"recommend\", \",\", \"wo\", \"better\", \"experience\", \"n't\", \"love\", \"'s\", \"product\", \"amazing\", \"event\", \"every\", \"\\ud83d\\ude0a\", \"moment\", \"enjoyed\", \"great\", \"!\", \"wo\", \"n't\", \"recommend\", \"terrible\", \"service\", \"quality\", \"\\ud83d\\ude21\", \"disappointed\", \"bad\", \"could\", \",\", \"better\", \"experience\", \"love\", \"product\", \"amazing\", \"'s\", \"event\", \"great\", \"every\", \"moment\", \"enjoyed\", \"\\ud83d\\ude0a\", \"!\", \"amazing\", \"'s\", \"product\", \"love\", \"!\", \"quality\", \"disappointed\", \"\\ud83d\\ude21\", \"terrible\", \",\", \"could\", \"service\", \"bad\", \"experience\", \"wo\", \"n't\", \"recommend\", \"better\", \"enjoyed\", \"great\", \"event\", \"every\", \"\\ud83d\\ude0a\", \"moment\"], \"Freq\": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.019097258983605224, 0.019097255568750533, 0.019097252153895843, 0.019097253861323188, 0.019097252153895843, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097248739041152, 0.019097250446468497, 0.019097250446468497, 0.019097248739041152, 0.019097248739041152, 0.019097252153895843, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.01909725727617788, 0.5310453704004567, 0.5310453704004567, 0.5310453300431472, 0.0531046349065213, 0.0531046349065213, 0.0531046349065213, 0.0531046349065213, 0.0531046349065213, 0.05310462986185761, 0.05310462986185761, 0.05310462986185761, 0.05310462986185761, 0.05310462481719391, 0.0531046349065213, 0.05310462986185761, 0.05310462986185761, 0.05310462986185761, 0.05310462986185761, 0.05310462986185761, 0.05310462481719391, 0.05310462481719391, 0.05310462481719391, 0.053104619772530226, 0.05310464499584868, 0.019097258983605224, 0.019097255568750533, 0.019097253861323188, 0.019097255568750533, 0.019097253861323188, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097253861323188, 0.019097250446468497, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097258983605224, 0.6702894324410344, 0.6702894324410344, 0.6702893635231767, 0.6702893635231767, 0.6702893635231767, 0.0670290776339259, 0.06702907332655979, 0.06702907332655979, 0.0670290776339259, 0.06702906471182758, 0.06702906471182758, 0.06702906471182758, 0.06702906040446148, 0.06702906471182758, 0.06702906901919368, 0.06702906471182758, 0.06702906471182758, 0.06702906471182758, 0.06702906471182758, 0.06702906040446148, 0.06702906040446148, 0.06702906040446148, 0.06702905609709538, 0.067029081941292, 0.7567043042461077, 0.7567043042461077, 0.7567043042461077, 0.7567043042461077, 0.7567043042461077, 0.7567042551964831, 0.7567031270551163, 0.0756706155869438, 0.07567060945574071, 0.07567060332453762, 0.07567060945574071, 0.07567060332453762, 0.07567060332453762, 0.07567060332453762, 0.07567060332453762, 0.07567059719333455, 0.07567059719333455, 0.07567059106213148, 0.07567059106213148, 0.07567059106213148, 0.07567060332453762, 0.07567060332453762, 0.07567059719333455, 0.07567059106213148, 0.01909725727617788, 0.01909725727617788, 0.019097252153895843, 0.019097253861323188, 0.019097253861323188, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097250446468497, 0.019097248739041152, 0.019097248739041152, 0.019097252153895843, 0.019097252153895843, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097253861323188, 0.019097255568750533, 0.019097255568750533, 0.019097255568750533, 0.019097255568750533, 0.019097253861323188, 0.019097253861323188, 0.019097253861323188, 0.019097253861323188, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097250446468497, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097253861323188, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097258983605224, 0.6702894324410344, 0.6702894324410344, 0.6702894324410344, 0.6702893635231767, 0.6702893635231767, 0.06702908624865811, 0.0670290776339259, 0.06702907332655979, 0.06702906901919368, 0.06702906901919368, 0.06702906471182758, 0.06702906471182758, 0.06702906040446148, 0.06702906471182758, 0.06702906901919368, 0.06702906471182758, 0.06702906040446148, 0.06702906901919368, 0.06702906471182758, 0.06702906040446148, 0.06702906040446148, 0.06702906040446148, 0.06702905609709538, 0.0670290776339259, 0.6702896936429594, 0.6702896936429594, 0.6702896936429594, 0.6702896247250536, 0.670290796329452, 0.06702912012221485, 0.0670291028927384, 0.0670291028927384, 0.06702910720010752, 0.06702909858536929, 0.06702909858536929, 0.06702909858536929, 0.06702909427800018, 0.06702909427800018, 0.06702909427800018, 0.06702909427800018, 0.06702909427800018, 0.06702908997063106, 0.06702909427800018, 0.06702909427800018, 0.06702908997063106, 0.06702908997063106, 0.06702908566326195, 0.06702908566326195], \"Total\": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8841922500084596, 0.8841922498532206, 0.8841922552859173, 1.009511801257454, 1.0095118253942104, 1.0095117709506645, 1.0095117719595885, 1.0095117796818094, 1.009511782281754, 1.0095118047886953, 1.0095118115407835, 1.0095118298179862, 1.009511809833359, 1.009512000995351, 1.0095120476003148, 1.0095120440690504, 1.0095120700684708, 1.0872851139607218, 1.0872851288618453, 1.0872851295991397, 1.087285133906506, 1.087285142521244, 1.0872851561806398, 1.6905457570603464, 0.8841922498532206, 0.8841922552859173, 0.8841922500084596, 1.0095117709506645, 1.0095117719595885, 1.0095117796818094, 1.009511782281754, 1.009511801257454, 1.009511809833359, 1.0095118115407835, 1.0095118253942104, 1.0095118298179862, 1.0095118047886953, 1.009512000995351, 1.0095120440690504, 1.0095120476003148, 1.0095120700684708, 1.0872851139607218, 1.0872851561806398, 1.0872851295991397, 1.087285133906506, 1.087285142521244, 1.0872851288618453, 1.6905457570603464, 0.8841922500084596, 0.8841922552859173, 0.8841922498532206, 1.009511801257454, 1.0095117709506645, 1.0095117719595885, 1.0095117796818094, 1.009511782281754, 1.009511809833359, 1.0095118115407835, 1.0095118253942104, 1.0095118047886953, 1.009512000995351, 1.0095118298179862, 1.0095120440690504, 1.0095120700684708, 1.0095120476003148, 1.0872851139607218, 1.0872851288618453, 1.0872851295991397, 1.087285133906506, 1.087285142521244, 1.0872851561806398, 1.6905457570603464, 1.0095118047886953, 1.0095118115407835, 1.0095117709506645, 1.0095117719595885, 1.0095117796818094, 0.8841922498532206, 0.8841922500084596, 0.8841922552859173, 1.009511801257454, 1.009511782281754, 1.0095118253942104, 1.0095118298179862, 1.009511809833359, 1.009512000995351, 1.0095120700684708, 1.0095120440690504, 1.0095120476003148, 1.0872851139607218, 1.0872851561806398, 1.0872851295991397, 1.087285133906506, 1.087285142521244, 1.0872851288618453, 1.6905457570603464, 1.0872851288618453, 1.0872851295991397, 1.087285133906506, 1.087285142521244, 1.0872851561806398, 1.0872851139607218, 1.6905457570603464, 0.8841922552859173, 0.8841922500084596, 0.8841922498532206, 1.009511801257454, 1.0095117719595885, 1.0095117796818094, 1.009511782281754, 1.0095118298179862, 1.0095117709506645, 1.0095118253942104, 1.0095118047886953, 1.009511809833359, 1.0095118115407835, 1.009512000995351, 1.0095120700684708, 1.0095120476003148, 1.0095120440690504, 0.8841922498532206, 0.8841922500084596, 0.8841922552859173, 1.0095117796818094, 1.009511801257454, 1.0095117709506645, 1.0095117719595885, 1.009511782281754, 1.0095118253942104, 1.0095118047886953, 1.0095118298179862, 1.009511809833359, 1.0095118115407835, 1.009512000995351, 1.0095120700684708, 1.0095120440690504, 1.0095120476003148, 1.0872851139607218, 1.0872851288618453, 1.0872851295991397, 1.087285133906506, 1.087285142521244, 1.0872851561806398, 1.6905457570603464, 0.8841922498532206, 0.8841922500084596, 0.8841922552859173, 1.0095117709506645, 1.0095117796818094, 1.009511782281754, 1.009511801257454, 1.0095118298179862, 1.0095117719595885, 1.009511809833359, 1.0095118115407835, 1.0095118047886953, 1.0095118253942104, 1.009512000995351, 1.0095120476003148, 1.0095120700684708, 1.0095120440690504, 1.0872851139607218, 1.0872851288618453, 1.0872851295991397, 1.087285133906506, 1.087285142521244, 1.0872851561806398, 1.6905457570603464, 1.009511809833359, 1.0095118253942104, 1.0095118298179862, 1.009511782281754, 1.009511801257454, 0.8841922500084596, 0.8841922552859173, 0.8841922498532206, 1.0095117709506645, 1.0095117796818094, 1.0095117719595885, 1.0095118115407835, 1.0095118047886953, 1.009512000995351, 1.0095120700684708, 1.0095120440690504, 1.0095120476003148, 1.0872851139607218, 1.0872851561806398, 1.0872851288618453, 1.087285133906506, 1.087285142521244, 1.0872851295991397, 1.6905457570603464, 1.0095120440690504, 1.0095120476003148, 1.0095120700684708, 1.009512000995351, 1.6905457570603464, 0.8841922500084596, 0.8841922498532206, 0.8841922552859173, 1.009511782281754, 1.0095117719595885, 1.0095117796818094, 1.009511801257454, 1.0095117709506645, 1.0095118047886953, 1.009511809833359, 1.0095118253942104, 1.0095118298179862, 1.0095118115407835, 1.087285142521244, 1.0872851561806398, 1.0872851139607218, 1.0872851288618453, 1.0872851295991397, 1.087285133906506], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\"], \"logprob\": [24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -1.6292, -1.6292, -1.6292, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -1.9315, -1.9315, -1.9315, -1.9315, -1.9315, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -2.1633, -2.1633, -2.1633, -2.1633, -2.1633, -2.1633, -2.1633, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -1.9315, -1.9315, -1.9315, -1.9315, -1.9315, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -1.9315, -1.9315, -1.9315, -1.9315, -1.9315, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341], \"loglift\": [24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.1639, 0.1639, 0.1639, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.4842, 1.7127, 1.7127, 1.7127, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7966, -0.7966, -0.7966, -0.7966, -0.7966, -0.7966, -1.238, 0.1639, 0.1639, 0.1639, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.4842, 1.2779, 1.2779, 1.2779, 1.2779, 1.2779, -0.8921, -0.8921, -0.8921, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.5403, 0.9719, 0.9719, 0.9719, 0.9719, 0.9719, 0.9719, 0.5305, -1.1239, -1.1239, -1.1239, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, 0.1639, 0.1639, 0.1639, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.4842, 0.1639, 0.1639, 0.1639, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.4842, 1.2779, 1.2779, 1.2779, 1.2779, 1.2779, -0.8921, -0.8921, -0.8921, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.5403, 1.2779, 1.2779, 1.2779, 1.2779, 0.7623, -0.8921, -0.8921, -0.8921, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989]}, \"token.table\": {\"Topic\": [5, 9, 9, 4, 9, 4, 4, 4, 2, 5, 5, 5, 4, 5, 9, 5, 8, 9, 2, 8, 8, 8, 8, 5, 2], \"Freq\": [0.5915249533019907, 0.5915249533019907, 0.9905775789175318, 0.9905778493883979, 0.9905775823825637, 0.9905778503783991, 0.9905778105495705, 0.9905778418110114, 1.1309757580051214, 0.9197219394363806, 0.9197219635953969, 0.9197219509907083, 0.9905778171750194, 0.9197219278820556, 0.9905776246483722, 0.9197219467234881, 0.9905777969559733, 0.9905775568707904, 1.1309757578065545, 0.9905777926151681, 0.9905778206400301, 0.9905778392598301, 0.9905778122249712, 0.9197219503670394, 1.1309757510561258], \"Term\": [\"!\", \"!\", \"'s\", \",\", \"amazing\", \"bad\", \"better\", \"could\", \"disappointed\", \"enjoyed\", \"event\", \"every\", \"experience\", \"great\", \"love\", \"moment\", \"n't\", \"product\", \"quality\", \"recommend\", \"service\", \"terrible\", \"wo\", \"\\ud83d\\ude0a\", \"\\ud83d\\ude21\"]}, \"R\": 24, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el58951386353809373767072921303\", ldavis_el58951386353809373767072921303_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el58951386353809373767072921303\", ldavis_el58951386353809373767072921303_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el58951386353809373767072921303\", ldavis_el58951386353809373767072921303_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I tried this question many times with no success\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\"\n",
        "]\n",
        "\n",
        "# Initialize BERTopic\n",
        "topic_model = BERTopic()\n",
        "\n",
        "# Find optimal number of topics\n",
        "optimal_k = topic_model.fit(documents)\n",
        "\n",
        "# Get the topics\n",
        "topics = topic_model.get_topics()\n",
        "\n",
        "# Summarize topics\n",
        "print(\"Summarizing Topics:\")\n",
        "for i, topic in enumerate(topics):\n",
        "    words, _ = zip(*topic)\n",
        "    words = \" \".join(words)\n",
        "    print(f\"Topic {i + 1}: {words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "3wmi1MwSAZ3r",
        "outputId": "6080cf59-5ccc-4f2f-f831-6cf6637d23a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
            "  warnings.warn(\"k >= N for N * N square matrix. \"\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1600: RuntimeWarning: k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
            "  warnings.warn(\"k >= N for N * N square matrix. \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m_reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   3354\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3355\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2779\u001b[0m             )\n\u001b[0;32m-> 2780\u001b[0;31m             self.embedding_, aux_data = self._fit_embed_data(\n\u001b[0m\u001b[1;32m   2781\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2825\u001b[0m         \"\"\"\n\u001b[0;32m-> 2826\u001b[0;31m         return simplicial_set_embedding(\n\u001b[0m\u001b[1;32m   2827\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spectral\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         embedding = spectral_layout(\n\u001b[0m\u001b[1;32m   1107\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/spectral.py\u001b[0m in \u001b[0;36mspectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, tol, maxiter)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m     return _spectral_layout(\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/spectral.py\u001b[0m in \u001b[0;36m_spectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, init, method, tol, maxiter)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eigsh\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(\n\u001b[0m\u001b[1;32m    522\u001b[0m                 \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m             raise TypeError(\"Cannot use scipy.linalg.eigh for sparse A with \"\n\u001b[0m\u001b[1;32m   1606\u001b[0m                             \u001b[0;34m\"k >= N. Use scipy.linalg.eigh(A.toarray()) or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-7da32bc40b27>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Find optimal number of topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moptimal_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Get the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# Reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mumap_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_dimensionality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;31m# Cluster reduced embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36m_reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   3356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3358\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m         \u001b[0mumap_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2778\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m             )\n\u001b[0;32m-> 2780\u001b[0;31m             self.embedding_, aux_data = self._fit_embed_data(\n\u001b[0m\u001b[1;32m   2781\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2782\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mreplaced\u001b[0m \u001b[0mby\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m         \"\"\"\n\u001b[0;32m-> 2826\u001b[0;31m         return simplicial_set_embedding(\n\u001b[0m\u001b[1;32m   2827\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/umap_.py\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spectral\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         embedding = spectral_layout(\n\u001b[0m\u001b[1;32m   1107\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/spectral.py\u001b[0m in \u001b[0;36mspectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, tol, maxiter)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mspectral\u001b[0m \u001b[0membedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m     return _spectral_layout(\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/umap/spectral.py\u001b[0m in \u001b[0;36m_spectral_layout\u001b[0;34m(data, graph, dim, random_state, metric, metric_kwds, init, method, tol, maxiter)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eigsh\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(\n\u001b[0m\u001b[1;32m    522\u001b[0m                 \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m             raise TypeError(\"Cannot use scipy.linalg.eigh for sparse A with \"\n\u001b[0m\u001b[1;32m   1606\u001b[0m                             \u001b[0;34m\"k >= N. Use scipy.linalg.eigh(A.toarray()) or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m                             \" reduce k.\")\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA:\n",
        "\"\"\"\n",
        "Topics: The topics generated by LDA include words like \"bad,\" \"experience,\" \"could,\" \"better,\" \"quality,\" \"disappointed,\" \"love,\" \"amazing,\" \"moment,\" \"product,\" \"recommend,\" and \"terrible.\"\n",
        "Coherence Score: The coherence score for the optimal number of topics (5 topics) is 0.5349.\n",
        "\"\"\"\n",
        "\n",
        "# LSA:\n",
        "\"\"\"\n",
        "Topics: The topics generated by LSA include words like \"enjoyed,\" \"moment,\" \"great,\" \"event,\" \"every,\" \"experience,\" \"bad,\" \"could,\" \"better,\" \"wo,\" \"terrible,\" and \"service.\"\n",
        "Coherence Score: The coherence score for the optimal number of topics (3 topics) is 0.5349.\n",
        "\"\"\"\n",
        "\n",
        "# lda2vec:\n",
        "\"\"\"\n",
        "Topics: The topics generated by lda2vec include words like \"quality,\" \"disappointed,\" \"service,\" \"event,\" \"love,\" \"could,\" \"better,\" \"amazing,\" \"recommend,\" \"bad,\" \"enjoyed,\" \"moment,\" \"great,\" \"every,\" \"wo,\" and \"n't.\"\n",
        "Coherence Score: The coherence scores for different numbers of topics range from 0.4931 to 0.5320, with the optimal number of topics being 9, achieving a coherence score of 0.5320.\n",
        "\"\"\"\n",
        "\n",
        "# BERTopic:\n",
        "# BERTopic's output was not obtained due to an error during execution, so we cannot directly assess the topics it generated.\n",
        "\n"
      ],
      "metadata": {
        "id": "OK34nZtojhmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09a6a54-372b-4738-e443-e0109eb66732"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Learning Experience\n",
        "# Working with text data and implementing various topic modeling algorithms was a valuable learning experience.\n",
        " It helped me in understanding the underlying concepts of feature extraction from text data and how different algorithms approach the task of\n",
        "  identifying topics within a corpus. Implementing algorithms such as LDA, LSA, and lda2vec provided insights into their strengths and\n",
        "  weaknesses in handling text data.\n",
        "\n",
        "# Challenges Encountered\n",
        "# The main challenge I encountered was generating topics using BERTopic due to inability to successfully execute the code due to errors.\n",
        " Despite several attempts, the BERTopic implementation did not yield the expected results.\n",
        "\n",
        "# Relevance to Your Field of Study\n",
        "# This exercise modeling plays a crucial role in various NLP applications such as document clustering, summarization, and recommendation systems.\n",
        " Understanding different topic modeling algorithms and their implementations is essential for NLP practitioners to effectively analyze and extract insights from large text datasets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
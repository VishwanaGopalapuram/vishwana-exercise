{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7a5e4d-e096-47d4-e73c-360064c7c682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best number of topics: 4 (Coherence Score: 0.5349)\n",
            "Topics:\n",
            "Topic 1: 0.187*\"disappointed\" + 0.187*\"quality\" + 0.040*\"amazing\" + 0.039*\"recommend\" + 0.039*\"wo\" + 0.039*\"bad\" + 0.039*\"product\" + 0.039*\"moment\" + 0.039*\"great\" + 0.039*\"could\"\n",
            "Topic 2: 0.164*\"love\" + 0.164*\"product\" + 0.162*\"amazing\" + 0.034*\"quality\" + 0.034*\"disappointed\" + 0.034*\"enjoyed\" + 0.034*\"recommend\" + 0.034*\"experience\" + 0.034*\"wo\" + 0.034*\"bad\"\n",
            "Topic 3: 0.092*\"event\" + 0.092*\"every\" + 0.092*\"service\" + 0.092*\"moment\" + 0.092*\"great\" + 0.092*\"terrible\" + 0.092*\"enjoyed\" + 0.092*\"wo\" + 0.092*\"recommend\" + 0.019*\"disappointed\"\n",
            "Topic 4: 0.145*\"better\" + 0.145*\"could\" + 0.145*\"experience\" + 0.145*\"bad\" + 0.031*\"disappointed\" + 0.031*\"amazing\" + 0.031*\"quality\" + 0.030*\"love\" + 0.030*\"terrible\" + 0.030*\"enjoyed\"\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\"\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lowercasing and removing punctuation\n",
        "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_docs = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "dictionary = corpora.Dictionary(preprocessed_docs)\n",
        "\n",
        "\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
        "\n",
        "\n",
        "coherence_scores = []\n",
        "for k in range(2, 10):\n",
        "    lda_model = models.LdaModel(corpus, num_topics=k, id2word=dictionary)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append((k, coherence_lda))\n",
        "\n",
        "best_k, best_coherence = max(coherence_scores, key=lambda x: x[1])\n",
        "print(f\"Best number of topics: {best_k} (Coherence Score: {best_coherence:.4f})\")\n",
        "\n",
        "# Train the LDA model with the best number of topics\n",
        "best_lda_model = models.LdaModel(corpus, num_topics=best_k, id2word=dictionary)\n",
        "\n",
        "# Print the topics\n",
        "print(\"Topics:\")\n",
        "for idx, topic in best_lda_model.print_topics():\n",
        "    print(f\"Topic {idx + 1}: {topic}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c30ddd4-1543-47b2-aaf0-45b030f5eb5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best number of topics: 5 (Coherence Score: 0.5349)\n",
            "Topics:\n",
            "Topic 1: 0.447*\"event\" + 0.447*\"moment\" + 0.447*\"every\" + 0.447*\"enjoyed\" + 0.447*\"great\" + 0.000*\"wo\" + 0.000*\"terrible\" + 0.000*\"service\" + 0.000*\"recommend\" + 0.000*\"bad\"\n",
            "Topic 2: 0.354*\"could\" + 0.354*\"better\" + -0.354*\"recommend\" + -0.354*\"service\" + -0.354*\"terrible\" + -0.354*\"wo\" + 0.354*\"bad\" + 0.354*\"experience\" + 0.000*\"quality\" + 0.000*\"disappointed\"\n",
            "Topic 3: 0.354*\"service\" + 0.354*\"wo\" + 0.354*\"terrible\" + 0.354*\"recommend\" + 0.354*\"could\" + 0.354*\"experience\" + 0.354*\"better\" + 0.354*\"bad\" + -0.000*\"every\" + -0.000*\"moment\"\n",
            "Topic 4: 0.577*\"amazing\" + 0.577*\"love\" + 0.577*\"product\" + -0.000*\"service\" + 0.000*\"could\" + 0.000*\"better\" + 0.000*\"bad\" + 0.000*\"disappointed\" + 0.000*\"experience\" + -0.000*\"every\"\n",
            "Topic 5: -0.707*\"quality\" + -0.707*\"disappointed\" + -0.000*\"wo\" + -0.000*\"terrible\" + -0.000*\"recommend\" + -0.000*\"service\" + 0.000*\"could\" + 0.000*\"better\" + 0.000*\"bad\" + 0.000*\"experience\"\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.models.lsimodel import LsiModel\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\"\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lowercasing and removing punctuation\n",
        "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_docs = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "dictionary = corpora.Dictionary(preprocessed_docs)\n",
        "\n",
        "# Create a corpus: a list of bag-of-words representation of each document\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
        "\n",
        "\n",
        "coherence_scores = []\n",
        "for k in range(2, 10):\n",
        "    lsi_model = LsiModel(corpus, num_topics=k, id2word=dictionary)\n",
        "    coherence_model_lsi = CoherenceModel(model=lsi_model, texts=preprocessed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_lsi = coherence_model_lsi.get_coherence()\n",
        "    coherence_scores.append((k, coherence_lsi))\n",
        "\n",
        "# Select the number of topics with the highest coherence score\n",
        "best_k, best_coherence = max(coherence_scores, key=lambda x: x[1])\n",
        "print(f\"Best number of topics: {best_k} (Coherence Score: {best_coherence:.4f})\")\n",
        "\n",
        "# Train the LSA model with the best number of topics\n",
        "best_lsi_model = LsiModel(corpus, num_topics=best_k, id2word=dictionary)\n",
        "\n",
        "# Print the topics\n",
        "print(\"Topics:\")\n",
        "for idx, topic in best_lsi_model.print_topics():\n",
        "    print(f\"Topic {idx + 1}: {topic}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/bmabey/pyLDAvis.git@master#egg=pyLDAvis\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\"\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.Defaults.stop_words |= {\"!\", \",\", \".\", \"'\", \"?\", \"(\", \")\", \":\", \";\", \"-\", \"ðŸ˜Š\", \"ðŸ˜¡\"}\n",
        "stop_words = set(stopwords.words('english'))\n",
        "preprocessed_docs = []\n",
        "for doc in documents:\n",
        "    tokens = nlp(doc)\n",
        "    tokens = [token.text.lower() for token in tokens if token.text.lower() not in stop_words]\n",
        "    preprocessed_docs.append(tokens)\n",
        "\n",
        "dictionary = corpora.Dictionary(preprocessed_docs)\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
        "\n",
        "# Calculate coherence score for different numbers of topics\n",
        "coherence_scores = []\n",
        "for num_topics in range(2, 11):\n",
        "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append((num_topics, coherence_score))\n",
        "\n",
        "# Print coherence scores\n",
        "print(\"Coherence Scores:\")\n",
        "for num_topics, score in coherence_scores:\n",
        "    print(f\"Number of Topics: {num_topics}, Coherence Score: {score}\")\n",
        "\n",
        "# Find the optimal number of topics based on coherence score\n",
        "optimal_num_topics, optimal_coherence_score = max(coherence_scores, key=lambda x: x[1])\n",
        "print(f\"\\nOptimal Number of Topics: {optimal_num_topics}, Coherence Score: {optimal_coherence_score}\")\n",
        "\n",
        "# Summarize topics for the optimal number of topics\n",
        "lda_model = models.LdaModel(corpus, num_topics=optimal_num_topics, id2word=dictionary, passes=15)\n",
        "print(\"\\nTopics:\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic {idx}: {topic}\")\n",
        "\n",
        "# Visualize the topics\n",
        "lda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4VNH99ZGoXqy",
        "outputId": "13ed61d4-f875-4da0-84a1-0165511e7faf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Cloning https://github.com/bmabey/pyLDAvis.git (to revision master) to /tmp/pip-install-yhhz_qtj/pyldavis_407dcd06c0624b56a9547397d7b74996\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/bmabey/pyLDAvis.git /tmp/pip-install-yhhz_qtj/pyldavis_407dcd06c0624b56a9547397d7b74996\n",
            "  Resolved https://github.com/bmabey/pyLDAvis.git to commit 16800f36bc95b4c99d8c26d51daa3485c8cb76da\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.9.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Coherence Scores:\n",
            "Number of Topics: 2, Coherence Score: 0.5008437441772384\n",
            "Number of Topics: 3, Coherence Score: 0.5085759020869889\n",
            "Number of Topics: 4, Coherence Score: 0.4892455073126128\n",
            "Number of Topics: 5, Coherence Score: 0.5203750373403819\n",
            "Number of Topics: 6, Coherence Score: 0.523025170348827\n",
            "Number of Topics: 7, Coherence Score: 0.524029427496954\n",
            "Number of Topics: 8, Coherence Score: 0.5287799185339404\n",
            "Number of Topics: 9, Coherence Score: 0.5314307035620589\n",
            "Number of Topics: 10, Coherence Score: 0.5300999195911433\n",
            "\n",
            "Optimal Number of Topics: 9, Coherence Score: 0.5314307035620589\n",
            "\n",
            "Topics:\n",
            "Topic 0: 0.145*\",\" + 0.145*\"bad\" + 0.145*\"could\" + 0.145*\"experience\" + 0.145*\"better\" + 0.014*\"disappointed\" + 0.014*\"quality\" + 0.014*\"!\" + 0.014*\"ðŸ˜¡\" + 0.014*\"service\"\n",
            "Topic 1: 0.115*\"enjoyed\" + 0.115*\"ðŸ˜Š\" + 0.115*\"moment\" + 0.115*\"great\" + 0.115*\"every\" + 0.115*\"event\" + 0.115*\"!\" + 0.011*\"disappointed\" + 0.011*\"ðŸ˜¡\" + 0.011*\"quality\"\n",
            "Topic 2: 0.042*\"disappointed\" + 0.042*\"quality\" + 0.042*\"service\" + 0.042*\"ðŸ˜¡\" + 0.042*\"!\" + 0.042*\"amazing\" + 0.042*\"terrible\" + 0.042*\"product\" + 0.042*\",\" + 0.042*\"recommend\"\n",
            "Topic 3: 0.042*\"ðŸ˜¡\" + 0.042*\"disappointed\" + 0.042*\"quality\" + 0.042*\"service\" + 0.042*\"!\" + 0.042*\"wo\" + 0.042*\"love\" + 0.042*\"better\" + 0.042*\"experience\" + 0.042*\"could\"\n",
            "Topic 4: 0.145*\"!\" + 0.145*\"'s\" + 0.145*\"amazing\" + 0.145*\"love\" + 0.145*\"product\" + 0.014*\"disappointed\" + 0.014*\"quality\" + 0.014*\"ðŸ˜¡\" + 0.014*\"service\" + 0.014*\",\"\n",
            "Topic 5: 0.145*\"n't\" + 0.145*\"recommend\" + 0.145*\"terrible\" + 0.145*\"wo\" + 0.145*\"service\" + 0.014*\"quality\" + 0.014*\"ðŸ˜¡\" + 0.014*\"disappointed\" + 0.014*\"!\" + 0.014*\"experience\"\n",
            "Topic 6: 0.196*\"disappointed\" + 0.196*\"ðŸ˜¡\" + 0.196*\"quality\" + 0.020*\"!\" + 0.020*\"service\" + 0.020*\"better\" + 0.020*\"could\" + 0.020*\"love\" + 0.020*\"n't\" + 0.020*\"product\"\n",
            "Topic 7: 0.042*\"!\" + 0.042*\"ðŸ˜¡\" + 0.042*\"better\" + 0.042*\"quality\" + 0.042*\"disappointed\" + 0.042*\"recommend\" + 0.042*\"could\" + 0.042*\"amazing\" + 0.042*\"terrible\" + 0.042*\"bad\"\n",
            "Topic 8: 0.042*\"quality\" + 0.042*\"ðŸ˜¡\" + 0.042*\"!\" + 0.042*\"disappointed\" + 0.042*\"terrible\" + 0.042*\"service\" + 0.042*\"wo\" + 0.042*\"better\" + 0.042*\"love\" + 0.042*\"amazing\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el5381378353422348964601459617\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el5381378353422348964601459617_data = {\"mdsDat\": {\"x\": [0.11003962709265913, -0.15316388131456105, 0.0018173821176094855, 0.001817384360334109, -0.10470795146570867, 0.1100396134791806, 0.030523065575835794, 0.0018173802231811781, 0.0018173799314693546], \"y\": [-0.15484117733540118, -9.06910398569762e-09, 2.9444981073112595e-10, -7.221960169753557e-10, -6.383463421511082e-09, 0.15484117099564065, 2.4681113694949618e-08, -1.7388431955190019e-09, -7.221965577399439e-10], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [18.499997191317284, 26.333317625894598, 1.8333362156526252, 1.8333362156526252, 18.50001068059392, 18.499997191317284, 10.833332448266415, 1.8333362156526252, 1.8333362156526252]}, \"tinfo\": {\"Term\": [\"\\ud83d\\ude21\", \"quality\", \"disappointed\", \"!\", \"could\", \"recommend\", \"bad\", \",\", \"service\", \"better\", \"n't\", \"terrible\", \"wo\", \"experience\", \"amazing\", \"love\", \"product\", \"'s\", \"enjoyed\", \"great\", \"moment\", \"every\", \"\\ud83d\\ude0a\", \"event\", \",\", \"bad\", \"could\", \"experience\", \"better\", \"disappointed\", \"quality\", \"\\ud83d\\ude21\", \"service\", \"wo\", \"terrible\", \"n't\", \"recommend\", \"product\", \"love\", \"'s\", \"amazing\", \"event\", \"moment\", \"every\", \"\\ud83d\\ude0a\", \"great\", \"enjoyed\", \"!\", \"enjoyed\", \"every\", \"\\ud83d\\ude0a\", \"great\", \"event\", \"moment\", \"!\", \"\\ud83d\\ude21\", \"disappointed\", \"quality\", \"service\", \"experience\", \"terrible\", \"better\", \"bad\", \"recommend\", \",\", \"n't\", \"could\", \"wo\", \"'s\", \"amazing\", \"love\", \"product\", \"disappointed\", \"quality\", \"\\ud83d\\ude21\", \"service\", \"terrible\", \",\", \"wo\", \"experience\", \"better\", \"n't\", \"bad\", \"could\", \"recommend\", \"amazing\", \"product\", \"'s\", \"love\", \"great\", \"enjoyed\", \"every\", \"\\ud83d\\ude0a\", \"event\", \"moment\", \"!\", \"\\ud83d\\ude21\", \"disappointed\", \"quality\", \"service\", \"wo\", \"experience\", \"better\", \"could\", \"terrible\", \"n't\", \"bad\", \"recommend\", \",\", \"amazing\", \"love\", \"'s\", \"product\", \"enjoyed\", \"every\", \"\\ud83d\\ude0a\", \"great\", \"event\", \"moment\", \"!\", \"'s\", \"amazing\", \"product\", \"love\", \"!\", \"disappointed\", \"quality\", \"\\ud83d\\ude21\", \"wo\", \"service\", \",\", \"experience\", \"terrible\", \"better\", \"n't\", \"bad\", \"could\", \"recommend\", \"\\ud83d\\ude0a\", \"great\", \"moment\", \"enjoyed\", \"every\", \"event\", \"n't\", \"recommend\", \"wo\", \"terrible\", \"service\", \"quality\", \"\\ud83d\\ude21\", \"disappointed\", \"experience\", \"better\", \"bad\", \"could\", \",\", \"'s\", \"product\", \"amazing\", \"love\", \"great\", \"event\", \"moment\", \"every\", \"\\ud83d\\ude0a\", \"enjoyed\", \"!\", \"quality\", \"\\ud83d\\ude21\", \"disappointed\", \"service\", \"wo\", \"terrible\", \"better\", \"n't\", \"could\", \"recommend\", \"experience\", \",\", \"bad\", \"product\", \"love\", \"'s\", \"amazing\", \"\\ud83d\\ude0a\", \"event\", \"moment\", \"every\", \"great\", \"enjoyed\", \"!\", \"\\ud83d\\ude21\", \"quality\", \"disappointed\", \"better\", \"recommend\", \"terrible\", \"could\", \"wo\", \"experience\", \"service\", \"bad\", \",\", \"n't\", \"amazing\", \"product\", \"'s\", \"love\", \"moment\", \"enjoyed\", \"every\", \"\\ud83d\\ude0a\", \"great\", \"event\", \"!\", \"quality\", \"\\ud83d\\ude21\", \"disappointed\", \"wo\", \"terrible\", \"better\", \"service\", \"experience\", \",\", \"bad\", \"could\", \"recommend\", \"n't\", \"love\", \"'s\", \"amazing\", \"product\", \"enjoyed\", \"every\", \"\\ud83d\\ude0a\", \"great\", \"event\", \"moment\", \"!\"], \"Freq\": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6702894324410344, 0.6702894324410344, 0.6702894324410344, 0.6702893635231767, 0.6702893635231767, 0.06702908624865811, 0.0670290776339259, 0.06702907332655979, 0.06702907332655979, 0.06702906901919368, 0.06702906901919368, 0.06702906901919368, 0.06702906471182758, 0.06702906901919368, 0.06702906901919368, 0.06702906471182758, 0.06702906471182758, 0.06702906471182758, 0.06702906471182758, 0.06702906040446148, 0.06702906040446148, 0.06702906040446148, 0.06702905609709538, 0.0670290776339259, 0.7567043042461077, 0.7567043042461077, 0.7567043042461077, 0.7567043042461077, 0.7567043042461077, 0.7567043042461077, 0.7567031270551163, 0.07567062171814687, 0.07567062171814687, 0.07567060945574071, 0.07567060945574071, 0.07567060332453762, 0.07567060332453762, 0.07567060332453762, 0.07567060332453762, 0.07567060332453762, 0.07567059719333455, 0.07567059719333455, 0.07567059719333455, 0.07567059106213148, 0.07567059719333455, 0.07567059719333455, 0.07567059719333455, 0.07567059106213148, 0.01909725727617788, 0.019097255568750533, 0.019097253861323188, 0.019097253861323188, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097248739041152, 0.019097250446468497, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097253861323188, 0.01909725727617788, 0.01909725727617788, 0.019097255568750533, 0.019097253861323188, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097248739041152, 0.019097252153895843, 0.019097252153895843, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097253861323188, 0.670289645511076, 0.670289645511076, 0.670289645511076, 0.670289645511076, 0.6702908860334185, 0.06702910927871422, 0.06702910066397574, 0.06702910066397574, 0.06702909204923725, 0.06702909204923725, 0.06702909204923725, 0.067029087741868, 0.067029087741868, 0.067029087741868, 0.067029087741868, 0.067029087741868, 0.067029087741868, 0.067029087741868, 0.067029087741868, 0.067029087741868, 0.067029087741868, 0.06702908343449876, 0.06702908343449876, 0.06702908343449876, 0.6702894324410344, 0.6702894324410344, 0.6702893635231767, 0.6702893635231767, 0.6702893635231767, 0.06702908624865811, 0.067029081941292, 0.067029081941292, 0.06702907332655979, 0.06702906901919368, 0.06702906471182758, 0.06702906471182758, 0.06702906040446148, 0.06702906901919368, 0.06702906901919368, 0.06702906471182758, 0.06702906471182758, 0.06702906471182758, 0.06702906471182758, 0.06702906471182758, 0.06702906040446148, 0.06702906040446148, 0.06702905609709538, 0.0670290776339259, 0.5310452949816176, 0.5310452949816176, 0.5310452949816176, 0.05310463644502496, 0.053104631400361604, 0.053104631400361604, 0.053104631400361604, 0.053104631400361604, 0.053104631400361604, 0.053104631400361604, 0.053104626355698244, 0.053104626355698244, 0.053104626355698244, 0.053104631400361604, 0.053104631400361604, 0.053104626355698244, 0.053104626355698244, 0.053104626355698244, 0.053104626355698244, 0.053104626355698244, 0.05310462131103488, 0.05310462131103488, 0.05310461626637153, 0.05310464148968832, 0.019097255568750533, 0.019097253861323188, 0.019097253861323188, 0.019097253861323188, 0.019097253861323188, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097248739041152, 0.019097248739041152, 0.019097252153895843, 0.019097250446468497, 0.019097248739041152, 0.019097248739041152, 0.019097250446468497, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.019097248739041152, 0.01909725727617788, 0.019097258983605224, 0.019097258983605224, 0.019097255568750533, 0.019097253861323188, 0.019097253861323188, 0.019097253861323188, 0.019097253861323188, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097253861323188, 0.019097252153895843, 0.019097252153895843, 0.019097252153895843, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097250446468497, 0.019097258983605224], \"Total\": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0095118102296399, 1.0095118180682672, 1.009511820396582, 1.009511759472569, 1.0095117653321484, 0.8841922181508584, 0.8841921929663475, 0.8841921983214488, 1.0095117868301773, 1.0095117539622567, 1.0095117636247208, 1.0095118178742388, 1.0095118265277854, 1.0095120095052577, 1.0095120113290947, 1.0095120028695768, 1.0095120070993473, 1.087285140123552, 1.0872851461383486, 1.087285126464156, 1.0872851358161888, 1.0872851367863188, 1.0872851128047607, 1.6905458338285044, 1.0872851128047607, 1.087285126464156, 1.0872851358161888, 1.0872851367863188, 1.087285140123552, 1.0872851461383486, 1.6905458338285044, 0.8841921983214488, 0.8841922181508584, 0.8841921929663475, 1.0095117868301773, 1.009511759472569, 1.0095117636247208, 1.0095117653321484, 1.0095118180682672, 1.0095118265277854, 1.0095118102296399, 1.0095118178742388, 1.009511820396582, 1.0095117539622567, 1.0095120028695768, 1.0095120070993473, 1.0095120113290947, 1.0095120095052577, 0.8841922181508584, 0.8841921929663475, 0.8841921983214488, 1.0095117868301773, 1.0095117636247208, 1.0095118102296399, 1.0095117539622567, 1.009511759472569, 1.0095117653321484, 1.0095118178742388, 1.0095118180682672, 1.009511820396582, 1.0095118265277854, 1.0095120070993473, 1.0095120095052577, 1.0095120028695768, 1.0095120113290947, 1.0872851367863188, 1.0872851128047607, 1.087285126464156, 1.0872851358161888, 1.087285140123552, 1.0872851461383486, 1.6905458338285044, 0.8841921983214488, 0.8841922181508584, 0.8841921929663475, 1.0095117868301773, 1.0095117539622567, 1.009511759472569, 1.0095117653321484, 1.009511820396582, 1.0095117636247208, 1.0095118178742388, 1.0095118180682672, 1.0095118265277854, 1.0095118102296399, 1.0095120070993473, 1.0095120113290947, 1.0095120028695768, 1.0095120095052577, 1.0872851128047607, 1.087285126464156, 1.0872851358161888, 1.0872851367863188, 1.087285140123552, 1.0872851461383486, 1.6905458338285044, 1.0095120028695768, 1.0095120070993473, 1.0095120095052577, 1.0095120113290947, 1.6905458338285044, 0.8841922181508584, 0.8841921929663475, 0.8841921983214488, 1.0095117539622567, 1.0095117868301773, 1.0095118102296399, 1.009511759472569, 1.0095117636247208, 1.0095117653321484, 1.0095118178742388, 1.0095118180682672, 1.009511820396582, 1.0095118265277854, 1.0872851358161888, 1.0872851367863188, 1.0872851461383486, 1.0872851128047607, 1.087285126464156, 1.087285140123552, 1.0095118178742388, 1.0095118265277854, 1.0095117539622567, 1.0095117636247208, 1.0095117868301773, 0.8841921929663475, 0.8841921983214488, 0.8841922181508584, 1.009511759472569, 1.0095117653321484, 1.0095118180682672, 1.009511820396582, 1.0095118102296399, 1.0095120028695768, 1.0095120095052577, 1.0095120070993473, 1.0095120113290947, 1.0872851367863188, 1.087285140123552, 1.0872851461383486, 1.087285126464156, 1.0872851358161888, 1.0872851128047607, 1.6905458338285044, 0.8841921929663475, 0.8841921983214488, 0.8841922181508584, 1.0095117868301773, 1.0095117539622567, 1.0095117636247208, 1.0095117653321484, 1.0095118178742388, 1.009511820396582, 1.0095118265277854, 1.009511759472569, 1.0095118102296399, 1.0095118180682672, 1.0095120095052577, 1.0095120113290947, 1.0095120028695768, 1.0095120070993473, 1.0872851358161888, 1.087285140123552, 1.0872851461383486, 1.087285126464156, 1.0872851367863188, 1.0872851128047607, 1.6905458338285044, 0.8841921983214488, 0.8841921929663475, 0.8841922181508584, 1.0095117653321484, 1.0095118265277854, 1.0095117636247208, 1.009511820396582, 1.0095117539622567, 1.009511759472569, 1.0095117868301773, 1.0095118180682672, 1.0095118102296399, 1.0095118178742388, 1.0095120070993473, 1.0095120095052577, 1.0095120028695768, 1.0095120113290947, 1.0872851461383486, 1.0872851128047607, 1.087285126464156, 1.0872851358161888, 1.0872851367863188, 1.087285140123552, 1.6905458338285044, 0.8841921929663475, 0.8841921983214488, 0.8841922181508584, 1.0095117539622567, 1.0095117636247208, 1.0095117653321484, 1.0095117868301773, 1.009511759472569, 1.0095118102296399, 1.0095118180682672, 1.009511820396582, 1.0095118265277854, 1.0095118178742388, 1.0095120113290947, 1.0095120028695768, 1.0095120070993473, 1.0095120095052577, 1.0872851128047607, 1.087285126464156, 1.0872851358161888, 1.0872851367863188, 1.087285140123552, 1.0872851461383486, 1.6905458338285044], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\"], \"logprob\": [24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.9315, -1.9315, -1.9315, -1.9315, -1.9315, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -2.1633, -2.1633, -2.1633, -2.1633, -2.1633, -2.1633, -2.1633, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -4.4659, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -1.9315, -1.9315, -1.9315, -1.9315, -1.9315, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -1.9315, -1.9315, -1.9315, -1.9315, -1.9315, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -4.2341, -1.6292, -1.6292, -1.6292, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.9318, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781, -3.1781], \"loglift\": [24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2779, 1.2779, 1.2779, 1.2779, 1.2779, -0.8921, -0.8921, -0.8921, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.5403, 0.9719, 0.9719, 0.9719, 0.9719, 0.9719, 0.9719, 0.5305, -1.1239, -1.1239, -1.1239, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, -1.2565, 0.1639, 0.1639, 0.1639, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.4842, 0.1639, 0.1639, 0.1639, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.4842, 1.2779, 1.2779, 1.2779, 1.2779, 0.7623, -0.8921, -0.8921, -0.8921, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, 1.2779, 1.2779, 1.2779, 1.2779, 1.2779, -0.8921, -0.8921, -0.8921, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0247, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.0989, -1.5403, 1.7127, 1.7127, 1.7127, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7224, -0.7966, -0.7966, -0.7966, -0.7966, -0.7966, -0.7966, -1.238, 0.1639, 0.1639, 0.1639, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.4842, 0.1639, 0.1639, 0.1639, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, 0.0314, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.0429, -0.4842]}, \"token.table\": {\"Topic\": [2, 5, 5, 1, 5, 1, 1, 1, 7, 2, 2, 2, 1, 2, 5, 2, 6, 5, 7, 6, 6, 6, 6, 2, 7], \"Freq\": [0.5915249264406776, 0.5915249264406776, 0.9905776228092994, 0.9905778118361229, 0.9905776186588624, 0.9905778041445138, 0.990577855891537, 0.9905778018598679, 1.13097579855581, 0.9197219645732111, 0.9197219414645607, 0.9197219530188859, 0.9905778616412169, 0.9197219442874875, 0.9905776145084481, 0.9197219363767136, 0.9905778043349031, 0.9905776162980772, 1.1309758307694764, 0.9905777958436591, 0.9905778347967151, 0.9905778575669408, 0.9905778670481807, 0.9197219451081093, 1.1309758239197325], \"Term\": [\"!\", \"!\", \"'s\", \",\", \"amazing\", \"bad\", \"better\", \"could\", \"disappointed\", \"enjoyed\", \"event\", \"every\", \"experience\", \"great\", \"love\", \"moment\", \"n't\", \"product\", \"quality\", \"recommend\", \"service\", \"terrible\", \"wo\", \"\\ud83d\\ude0a\", \"\\ud83d\\ude21\"]}, \"R\": 24, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el5381378353422348964601459617\", ldavis_el5381378353422348964601459617_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el5381378353422348964601459617\", ldavis_el5381378353422348964601459617_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el5381378353422348964601459617\", ldavis_el5381378353422348964601459617_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I tried this question many times with no success\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"I love this product! It's amazing.\",\n",
        "    \"The service was terrible. I won't recommend it.\",\n",
        "    \"Not a bad experience, but could be better.\",\n",
        "    \"ðŸ˜Š Great event! Enjoyed every moment.\",\n",
        "    \"Disappointed with the quality. ðŸ˜¡\"\n",
        "]\n",
        "\n",
        "# Initialize BERTopic\n",
        "topic_model = BERTopic()\n",
        "\n",
        "# Find optimal number of topics\n",
        "optimal_k = topic_model.fit(documents)\n",
        "\n",
        "# Get the topics\n",
        "topics = topic_model.get_topics()\n",
        "\n",
        "# Summarize topics\n",
        "print(\"Summarizing Topics:\")\n",
        "for i, topic in enumerate(topics):\n",
        "    words, _ = zip(*topic)\n",
        "    words = \" \".join(words)\n",
        "    print(f\"Topic {i + 1}: {words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "3wmi1MwSAZ3r",
        "outputId": "d3bd27bc-c56e-4450-87fc-9f1b899e6acc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bertopic'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9ac5eb828771>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# I tried this question many times with no success\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Sample text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m documents = [\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA:\n",
        "\"\"\"\n",
        "Topics: The topics generated by LDA include words like \"bad,\" \"experience,\" \"could,\" \"better,\" \"quality,\" \"disappointed,\" \"love,\" \"amazing,\" \"moment,\" \"product,\" \"recommend,\" and \"terrible.\"\n",
        "Coherence Score: The coherence score for the optimal number of topics (5 topics) is 0.5349.\n",
        "\"\"\"\n",
        "\n",
        "# LSA:\n",
        "\"\"\"\n",
        "Topics: The topics generated by LSA include words like \"enjoyed,\" \"moment,\" \"great,\" \"event,\" \"every,\" \"experience,\" \"bad,\" \"could,\" \"better,\" \"wo,\" \"terrible,\" and \"service.\"\n",
        "Coherence Score: The coherence score for the optimal number of topics (3 topics) is 0.5349.\n",
        "\"\"\"\n",
        "\n",
        "# lda2vec:\n",
        "\"\"\"\n",
        "Topics: The topics generated by lda2vec include words like \"quality,\" \"disappointed,\" \"service,\" \"event,\" \"love,\" \"could,\" \"better,\" \"amazing,\" \"recommend,\" \"bad,\" \"enjoyed,\" \"moment,\" \"great,\" \"every,\" \"wo,\" and \"n't.\"\n",
        "Coherence Score: The coherence scores for different numbers of topics range from 0.4931 to 0.5320, with the optimal number of topics being 9, achieving a coherence score of 0.5320.\n",
        "\"\"\"\n",
        "\n",
        "# BERTopic:\n",
        "# BERTopic's output was not obtained due to an error during execution, so we cannot directly assess the topics it generated.\n",
        "\n"
      ],
      "metadata": {
        "id": "OK34nZtojhmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09a6a54-372b-4738-e443-e0109eb66732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Learning Experience\n",
        "# Working with text data and implementing various topic modeling algorithms was a valuable learning experience.\n",
        " It helped me in understanding the underlying concepts of feature extraction from text data and how different algorithms approach the task of\n",
        "  identifying topics within a corpus. Implementing algorithms such as LDA, LSA, and lda2vec provided insights into their strengths and\n",
        "  weaknesses in handling text data.\n",
        "\n",
        "# Challenges Encountered\n",
        "# The main challenge I encountered was generating topics using BERTopic due to inability to successfully execute the code due to errors.\n",
        " Despite several attempts, the BERTopic implementation did not yield the expected results.\n",
        "\n",
        "# Relevance to Your Field of Study\n",
        "# This exercise modeling plays a crucial role in various NLP applications such as document clustering, summarization, and recommendation systems.\n",
        " Understanding different topic modeling algorithms and their implementations is essential for NLP practitioners to effectively analyze and extract insights from large text datasets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
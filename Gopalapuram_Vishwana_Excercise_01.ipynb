{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "b78db21e-0316-4811-a074-a3e215fa1345"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Research Question:\\n## How does the implementation of remote work policies impact employee productivity and job satisfaction across various industries?\\n\\n# Data Needed:\\n\\n## Employee productivity metrics:\\nThis could include measures such as project completion rates, task turnaround time, and key performance indicators (KPIs) specific to each industry.\\n\\n## Employee job satisfaction surveys:\\nCollect data on employee satisfaction levels through surveys covering aspects like work-life balance, job autonomy, communication effectiveness, and overall job satisfaction.\\n\\n## Industry-specific data:\\nGather information on the nature of the industry, its size, growth rate, and any relevant contextual factors that may influence remote work effectiveness.\\n\\n## Remote work policy details:\\nCollect information on the specifics of remote work policies implemented by each organization, including flexibility options, communication tools provided, and support systems available for remote employees.\\n\\n# Amount of Data Needed:\\nTo ensure a comprehensive analysis, aim to collect data from a diverse range of industries and organizations. Depending on the scope of the study, a dataset consisting of at least 100 organizations across various industries would be sufficient.\\n\\n# Steps for Collecting and Saving the Data:\\n\\n##1. Define Industry Categories:\\nIdentify different industry sectors to ensure a varied representation in the dataset. Examples could include technology, finance, healthcare, manufacturing, and education.\\n\\n##2. Select Organizations:\\nChoose a diverse set of organizations within each industry sector, ranging from small businesses to large corporations, to capture a broad spectrum of remote work implementations.\\n\\n##3. Collect Productivity Metrics:\\nReach out to selected organizations to gather data on employee productivity metrics. This may involve accessing internal performance tracking systems or collaborating with HR departments to obtain relevant data.\\n\\n##4. Conduct Job Satisfaction Surveys:\\nDesign and distribute surveys to employees within each organization to assess job satisfaction levels. Ensure the surveys cover relevant aspects of remote work and job satisfaction tailored to each industry.\\n\\n##5. Gather Industry-Specific Data:\\nResearch and compile industry-specific data such as market trends, growth projections, and any external factors that may influence remote work effectiveness within each sector.\\n\\n##6. Document Remote Work Policies:\\nObtain detailed information on remote work policies from each organization, including policy documents, employee handbooks, or direct communication with HR representatives.\\n\\n##7. Organize and Analyze Data:\\nCollate the collected data into a structured format, organizing it by industry sector and organization. We need to conduct statistical analysis to identify correlations between remote work policies, productivity metrics, and job satisfaction levels.\\n\\n##8. Store Data Securely:\\nSave the collected data in a secure and accessible format, ensuring compliance with data protection regulations. \\n\\n##9. Interpret Results:\\nAnalyze the findings to draw conclusions about the impact of remote work policies on employee productivity and job satisfaction across different industries. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# write your answer here\n",
        "\"\"\"\n",
        "# Research Question:\n",
        "## How does the implementation of remote work policies impact employee productivity and job satisfaction across various industries?\n",
        "\n",
        "# Data Needed:\n",
        "\n",
        "## Employee productivity metrics:\n",
        "This could include measures such as project completion rates, task turnaround time, and key performance indicators (KPIs) specific to each industry.\n",
        "\n",
        "## Employee job satisfaction surveys:\n",
        "Collect data on employee satisfaction levels through surveys covering aspects like work-life balance, job autonomy, communication effectiveness, and overall job satisfaction.\n",
        "\n",
        "## Industry-specific data:\n",
        "Gather information on the nature of the industry, its size, growth rate, and any relevant contextual factors that may influence remote work effectiveness.\n",
        "\n",
        "## Remote work policy details:\n",
        "Collect information on the specifics of remote work policies implemented by each organization, including flexibility options, communication tools provided, and support systems available for remote employees.\n",
        "\n",
        "# Amount of Data Needed:\n",
        "To ensure a comprehensive analysis, aim to collect data from a diverse range of industries and organizations. Depending on the scope of the study, a dataset consisting of at least 100 organizations across various industries would be sufficient.\n",
        "\n",
        "# Steps for Collecting and Saving the Data:\n",
        "\n",
        "##1. Define Industry Categories:\n",
        "Identify different industry sectors to ensure a varied representation in the dataset. Examples could include technology, finance, healthcare, manufacturing, and education.\n",
        "\n",
        "##2. Select Organizations:\n",
        "Choose a diverse set of organizations within each industry sector, ranging from small businesses to large corporations, to capture a broad spectrum of remote work implementations.\n",
        "\n",
        "##3. Collect Productivity Metrics:\n",
        "Reach out to selected organizations to gather data on employee productivity metrics. This may involve accessing internal performance tracking systems or collaborating with HR departments to obtain relevant data.\n",
        "\n",
        "##4. Conduct Job Satisfaction Surveys:\n",
        "Design and distribute surveys to employees within each organization to assess job satisfaction levels. Ensure the surveys cover relevant aspects of remote work and job satisfaction tailored to each industry.\n",
        "\n",
        "##5. Gather Industry-Specific Data:\n",
        "Research and compile industry-specific data such as market trends, growth projections, and any external factors that may influence remote work effectiveness within each sector.\n",
        "\n",
        "##6. Document Remote Work Policies:\n",
        "Obtain detailed information on remote work policies from each organization, including policy documents, employee handbooks, or direct communication with HR representatives.\n",
        "\n",
        "##7. Organize and Analyze Data:\n",
        "Collate the collected data into a structured format, organizing it by industry sector and organization. We need to conduct statistical analysis to identify correlations between remote work policies, productivity metrics, and job satisfaction levels.\n",
        "\n",
        "##8. Store Data Securely:\n",
        "Save the collected data in a secure and accessible format, ensuring compliance with data protection regulations.\n",
        "\n",
        "##9. Interpret Results:\n",
        "Analyze the findings to draw conclusions about the impact of remote work policies on employee productivity and job satisfaction across different industries.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XvRknixTh1g",
        "outputId": "3d0b6b41-2609-49aa-e033-6d8a79cafe63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset of 1000 samples collected and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Generate dataset samples\n",
        "def generate_sample():\n",
        "    industry = random.choice([\"Technology\", \"Finance\", \"Healthcare\", \"Manufacturing\", \"Education\"])\n",
        "    organization_size = random.choice([\"Small\", \"Medium\", \"Large\"])\n",
        "    remote_work_policy = random.choice([\"Flexible hours\", \"Remote-first\", \"Hybrid\", \"Traditional office\"])\n",
        "    productivity_metric = random.uniform(0, 100)\n",
        "    job_satisfaction_score = random.uniform(0, 10)\n",
        "    return {\n",
        "        \"Industry\": industry,\n",
        "        \"Organization Size\": organization_size,\n",
        "        \"Remote Work Policy\": remote_work_policy,\n",
        "        \"Productivity Metric\": productivity_metric,\n",
        "        \"Job Satisfaction Score\": job_satisfaction_score\n",
        "    }\n",
        "\n",
        "# Collect 1000 samples\n",
        "data = []\n",
        "for _ in range(1000):\n",
        "    data.append(generate_sample())\n",
        "\n",
        "# Convert to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to CSV file\n",
        "df.to_csv(\"remote_work_dataset.csv\", index=False)\n",
        "\n",
        "print(\"Dataset of 1000 samples collected and saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"remote_work_dataset.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"First few rows of the dataset:\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Fe4wysITfg_4",
        "outputId": "b63a39d5-382a-46e0-cea6-26da080c9873"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Industry Organization Size  Remote Work Policy  Productivity Metric  \\\n",
              "0        Finance             Large  Traditional office            89.760712   \n",
              "1      Education            Medium  Traditional office            72.509566   \n",
              "2  Manufacturing            Medium      Flexible hours            99.861198   \n",
              "3        Finance            Medium      Flexible hours            89.518022   \n",
              "4     Technology            Medium  Traditional office            58.308474   \n",
              "\n",
              "   Job Satisfaction Score  \n",
              "0                4.856072  \n",
              "1                7.668754  \n",
              "2                6.247762  \n",
              "3                6.935025  \n",
              "4                1.336276  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57d5d80b-5822-462b-84bb-8e889d7bd5f3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Industry</th>\n",
              "      <th>Organization Size</th>\n",
              "      <th>Remote Work Policy</th>\n",
              "      <th>Productivity Metric</th>\n",
              "      <th>Job Satisfaction Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Finance</td>\n",
              "      <td>Large</td>\n",
              "      <td>Traditional office</td>\n",
              "      <td>89.760712</td>\n",
              "      <td>4.856072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Education</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Traditional office</td>\n",
              "      <td>72.509566</td>\n",
              "      <td>7.668754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Manufacturing</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Flexible hours</td>\n",
              "      <td>99.861198</td>\n",
              "      <td>6.247762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Finance</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Flexible hours</td>\n",
              "      <td>89.518022</td>\n",
              "      <td>6.935025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Technology</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Traditional office</td>\n",
              "      <td>58.308474</td>\n",
              "      <td>1.336276</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57d5d80b-5822-462b-84bb-8e889d7bd5f3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57d5d80b-5822-462b-84bb-8e889d7bd5f3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57d5d80b-5822-462b-84bb-8e889d7bd5f3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8e3d8df8-84e9-46bb-9f22-14fd9f6b84f0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8e3d8df8-84e9-46bb-9f22-14fd9f6b84f0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8e3d8df8-84e9-46bb-9f22-14fd9f6b84f0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Industry\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"Education\",\n          \"Healthcare\",\n          \"Manufacturing\"\n        ],\n        \"num_unique_values\": 5,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Organization Size\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"Large\",\n          \"Medium\",\n          \"Small\"\n        ],\n        \"num_unique_values\": 3,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Remote Work Policy\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"Flexible hours\",\n          \"Remote-first\",\n          \"Traditional office\"\n        ],\n        \"num_unique_values\": 4,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Productivity Metric\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.696247343275125,\n        \"min\": 0.0290410517856054,\n        \"max\": 99.87110753771977,\n        \"samples\": [\n          94.11480166370448,\n          98.779180652146,\n          64.18230155376108\n        ],\n        \"num_unique_values\": 1000,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Job Satisfaction Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.8145355774326575,\n        \"min\": 0.0440920926366072,\n        \"max\": 9.993519937661883,\n        \"samples\": [\n          3.72936394567383,\n          2.320072608876882,\n          9.962851454559264\n        ],\n        \"num_unique_values\": 1000,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display summary statistics\n",
        "print(\"\\nSummary statistics of the dataset:\")\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uluTG_K_fq59",
        "outputId": "638852e4-8640-4b2a-db59-ed408b348d63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary statistics of the dataset:\n",
            "       Productivity Metric  Job Satisfaction Score\n",
            "count          1000.000000             1000.000000\n",
            "mean             49.206720                5.016087\n",
            "std              28.696247                2.814536\n",
            "min               0.029041                0.044092\n",
            "25%              24.859808                2.611091\n",
            "50%              48.053481                4.975301\n",
            "75%              73.347214                7.414166\n",
            "max              99.871108                9.993520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_acm_articles(keyword, num_articles, num_pages):\n",
        "    base_url = \"https://dl.acm.org/action/doSearch\"\n",
        "    keyword = keyword.replace(\" \", \"+\")\n",
        "    articles = []\n",
        "\n",
        "    try:\n",
        "        page_count = 0\n",
        "        while page_count < num_pages and len(articles) < num_articles:\n",
        "            page_count += 1\n",
        "            search_url = f\"{base_url}?AllField={keyword}&startPage={page_count}\"\n",
        "            response = requests.get(search_url)\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Failed to fetch {search_url}. Status code: {response.status_code}\")\n",
        "                continue  # Continue to the next iteration of the loop\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            article_blocks = soup.find_all('div', class_='issue-item__content')\n",
        "\n",
        "            for block in article_blocks:\n",
        "                if len(articles) == num_articles:\n",
        "                    break\n",
        "\n",
        "                title_element = block.find('h5', class_='issue-item__title')\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                else:\n",
        "                    title = \"No title available\"\n",
        "\n",
        "                venue_element = block.find('span', class_='issue-item__detail')\n",
        "                if venue_element:\n",
        "                    venue = venue_element.text.strip()\n",
        "                else:\n",
        "                    venue = \"No venue information available\"\n",
        "\n",
        "                authors_element = block.find('div', class_='issue-item__authors')\n",
        "                if authors_element:\n",
        "                    authors = authors_element.text.strip()\n",
        "                else:\n",
        "                    authors = \"No author information available\"\n",
        "\n",
        "                abstract_element = block.find('div', class_='abstractFull')\n",
        "                if abstract_element:\n",
        "                    abstract = abstract_element.text.strip()\n",
        "                else:\n",
        "                    abstract = \"No abstract available\"\n",
        "\n",
        "                year_element = block.find('div', class_='issue-item__date')\n",
        "                if year_element:\n",
        "                    year = year_element.text.strip()\n",
        "                else:\n",
        "                    year = \"No publication year available\"\n",
        "\n",
        "                article_info = {\n",
        "                    'title': title,\n",
        "                    'venue': venue,\n",
        "                    'year': year,\n",
        "                    'authors': authors,\n",
        "                    'abstract': abstract\n",
        "                }\n",
        "                articles.append(article_info)\n",
        "                print(f\"Collected article {len(articles)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "keyword = \"XYZ\"\n",
        "num_articles = 1000\n",
        "num_pages = 100  # Number of pages to scrape\n",
        "articles = scrape_acm_articles(keyword, num_articles, num_pages)\n",
        "\n",
        "# Print the number of articles collected\n",
        "print(f\"Number of articles collected: {len(articles)}\")\n",
        "\n",
        "# Print the first few articles to verify\n",
        "for i in range(min(5, len(articles))):\n",
        "    print(f\"\\nArticle {i+1}:\")\n",
        "    print(articles[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14vLKn-o5vrR",
        "outputId": "3e56d0c3-e55f-4d72-f35e-93cc54a31b20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected article 1\n",
            "Collected article 2\n",
            "Collected article 3\n",
            "Collected article 4\n",
            "Collected article 5\n",
            "Collected article 6\n",
            "Collected article 7\n",
            "Collected article 8\n",
            "Collected article 9\n",
            "Collected article 10\n",
            "Collected article 11\n",
            "Collected article 12\n",
            "Collected article 13\n",
            "Collected article 14\n",
            "Collected article 15\n",
            "Collected article 16\n",
            "Collected article 17\n",
            "Collected article 18\n",
            "Collected article 19\n",
            "Collected article 20\n",
            "Collected article 21\n",
            "Collected article 22\n",
            "Collected article 23\n",
            "Collected article 24\n",
            "Collected article 25\n",
            "Collected article 26\n",
            "Collected article 27\n",
            "Collected article 28\n",
            "Collected article 29\n",
            "Collected article 30\n",
            "Collected article 31\n",
            "Collected article 32\n",
            "Collected article 33\n",
            "Collected article 34\n",
            "Collected article 35\n",
            "Collected article 36\n",
            "Collected article 37\n",
            "Collected article 38\n",
            "Collected article 39\n",
            "Collected article 40\n",
            "Collected article 41\n",
            "Collected article 42\n",
            "Collected article 43\n",
            "Collected article 44\n",
            "Collected article 45\n",
            "Collected article 46\n",
            "Collected article 47\n",
            "Collected article 48\n",
            "Collected article 49\n",
            "Collected article 50\n",
            "Collected article 51\n",
            "Collected article 52\n",
            "Collected article 53\n",
            "Collected article 54\n",
            "Collected article 55\n",
            "Collected article 56\n",
            "Collected article 57\n",
            "Collected article 58\n",
            "Collected article 59\n",
            "Collected article 60\n",
            "Collected article 61\n",
            "Collected article 62\n",
            "Collected article 63\n",
            "Collected article 64\n",
            "Collected article 65\n",
            "Collected article 66\n",
            "Collected article 67\n",
            "Collected article 68\n",
            "Collected article 69\n",
            "Collected article 70\n",
            "Collected article 71\n",
            "Collected article 72\n",
            "Collected article 73\n",
            "Collected article 74\n",
            "Collected article 75\n",
            "Collected article 76\n",
            "Collected article 77\n",
            "Collected article 78\n",
            "Collected article 79\n",
            "Collected article 80\n",
            "Collected article 81\n",
            "Collected article 82\n",
            "Collected article 83\n",
            "Collected article 84\n",
            "Collected article 85\n",
            "Collected article 86\n",
            "Collected article 87\n",
            "Collected article 88\n",
            "Collected article 89\n",
            "Collected article 90\n",
            "Collected article 91\n",
            "Collected article 92\n",
            "Collected article 93\n",
            "Collected article 94\n",
            "Collected article 95\n",
            "Collected article 96\n",
            "Collected article 97\n",
            "Collected article 98\n",
            "Collected article 99\n",
            "Collected article 100\n",
            "Collected article 101\n",
            "Collected article 102\n",
            "Collected article 103\n",
            "Collected article 104\n",
            "Collected article 105\n",
            "Collected article 106\n",
            "Collected article 107\n",
            "Collected article 108\n",
            "Collected article 109\n",
            "Collected article 110\n",
            "Collected article 111\n",
            "Collected article 112\n",
            "Collected article 113\n",
            "Collected article 114\n",
            "Collected article 115\n",
            "Collected article 116\n",
            "Collected article 117\n",
            "Collected article 118\n",
            "Collected article 119\n",
            "Collected article 120\n",
            "Collected article 121\n",
            "Collected article 122\n",
            "Collected article 123\n",
            "Collected article 124\n",
            "Collected article 125\n",
            "Collected article 126\n",
            "Collected article 127\n",
            "Collected article 128\n",
            "Collected article 129\n",
            "Collected article 130\n",
            "Collected article 131\n",
            "Collected article 132\n",
            "Collected article 133\n",
            "Collected article 134\n",
            "Collected article 135\n",
            "Collected article 136\n",
            "Collected article 137\n",
            "Collected article 138\n",
            "Collected article 139\n",
            "Collected article 140\n",
            "Collected article 141\n",
            "Collected article 142\n",
            "Collected article 143\n",
            "Collected article 144\n",
            "Collected article 145\n",
            "Collected article 146\n",
            "Collected article 147\n",
            "Collected article 148\n",
            "Collected article 149\n",
            "Collected article 150\n",
            "Collected article 151\n",
            "Collected article 152\n",
            "Collected article 153\n",
            "Collected article 154\n",
            "Collected article 155\n",
            "Collected article 156\n",
            "Collected article 157\n",
            "Collected article 158\n",
            "Collected article 159\n",
            "Collected article 160\n",
            "Collected article 161\n",
            "Collected article 162\n",
            "Collected article 163\n",
            "Collected article 164\n",
            "Collected article 165\n",
            "Collected article 166\n",
            "Collected article 167\n",
            "Collected article 168\n",
            "Collected article 169\n",
            "Collected article 170\n",
            "Collected article 171\n",
            "Collected article 172\n",
            "Collected article 173\n",
            "Collected article 174\n",
            "Collected article 175\n",
            "Collected article 176\n",
            "Collected article 177\n",
            "Collected article 178\n",
            "Collected article 179\n",
            "Collected article 180\n",
            "Collected article 181\n",
            "Collected article 182\n",
            "Collected article 183\n",
            "Collected article 184\n",
            "Collected article 185\n",
            "Collected article 186\n",
            "Collected article 187\n",
            "Collected article 188\n",
            "Collected article 189\n",
            "Collected article 190\n",
            "Collected article 191\n",
            "Collected article 192\n",
            "Collected article 193\n",
            "Collected article 194\n",
            "Collected article 195\n",
            "Collected article 196\n",
            "Collected article 197\n",
            "Collected article 198\n",
            "Collected article 199\n",
            "Collected article 200\n",
            "Collected article 201\n",
            "Collected article 202\n",
            "Collected article 203\n",
            "Collected article 204\n",
            "Collected article 205\n",
            "Collected article 206\n",
            "Collected article 207\n",
            "Collected article 208\n",
            "Collected article 209\n",
            "Collected article 210\n",
            "Collected article 211\n",
            "Collected article 212\n",
            "Collected article 213\n",
            "Collected article 214\n",
            "Collected article 215\n",
            "Collected article 216\n",
            "Collected article 217\n",
            "Collected article 218\n",
            "Collected article 219\n",
            "Collected article 220\n",
            "Collected article 221\n",
            "Collected article 222\n",
            "Collected article 223\n",
            "Collected article 224\n",
            "Collected article 225\n",
            "Collected article 226\n",
            "Collected article 227\n",
            "Collected article 228\n",
            "Collected article 229\n",
            "Collected article 230\n",
            "Collected article 231\n",
            "Collected article 232\n",
            "Collected article 233\n",
            "Collected article 234\n",
            "Collected article 235\n",
            "Collected article 236\n",
            "Collected article 237\n",
            "Collected article 238\n",
            "Collected article 239\n",
            "Collected article 240\n",
            "Collected article 241\n",
            "Collected article 242\n",
            "Collected article 243\n",
            "Collected article 244\n",
            "Collected article 245\n",
            "Collected article 246\n",
            "Collected article 247\n",
            "Collected article 248\n",
            "Collected article 249\n",
            "Collected article 250\n",
            "Collected article 251\n",
            "Collected article 252\n",
            "Collected article 253\n",
            "Collected article 254\n",
            "Collected article 255\n",
            "Collected article 256\n",
            "Collected article 257\n",
            "Collected article 258\n",
            "Collected article 259\n",
            "Collected article 260\n",
            "Collected article 261\n",
            "Collected article 262\n",
            "Collected article 263\n",
            "Collected article 264\n",
            "Collected article 265\n",
            "Collected article 266\n",
            "Collected article 267\n",
            "Collected article 268\n",
            "Collected article 269\n",
            "Collected article 270\n",
            "Collected article 271\n",
            "Collected article 272\n",
            "Collected article 273\n",
            "Collected article 274\n",
            "Collected article 275\n",
            "Collected article 276\n",
            "Collected article 277\n",
            "Collected article 278\n",
            "Collected article 279\n",
            "Collected article 280\n",
            "Collected article 281\n",
            "Collected article 282\n",
            "Collected article 283\n",
            "Collected article 284\n",
            "Collected article 285\n",
            "Collected article 286\n",
            "Collected article 287\n",
            "Collected article 288\n",
            "Collected article 289\n",
            "Collected article 290\n",
            "Collected article 291\n",
            "Collected article 292\n",
            "Collected article 293\n",
            "Collected article 294\n",
            "Collected article 295\n",
            "Collected article 296\n",
            "Collected article 297\n",
            "Collected article 298\n",
            "Collected article 299\n",
            "Collected article 300\n",
            "Collected article 301\n",
            "Collected article 302\n",
            "Collected article 303\n",
            "Collected article 304\n",
            "Collected article 305\n",
            "Collected article 306\n",
            "Collected article 307\n",
            "Collected article 308\n",
            "Collected article 309\n",
            "Collected article 310\n",
            "Collected article 311\n",
            "Collected article 312\n",
            "Collected article 313\n",
            "Collected article 314\n",
            "Collected article 315\n",
            "Collected article 316\n",
            "Collected article 317\n",
            "Collected article 318\n",
            "Collected article 319\n",
            "Collected article 320\n",
            "Collected article 321\n",
            "Collected article 322\n",
            "Collected article 323\n",
            "Collected article 324\n",
            "Collected article 325\n",
            "Collected article 326\n",
            "Collected article 327\n",
            "Collected article 328\n",
            "Collected article 329\n",
            "Collected article 330\n",
            "Collected article 331\n",
            "Collected article 332\n",
            "Collected article 333\n",
            "Collected article 334\n",
            "Collected article 335\n",
            "Collected article 336\n",
            "Collected article 337\n",
            "Collected article 338\n",
            "Collected article 339\n",
            "Collected article 340\n",
            "Collected article 341\n",
            "Collected article 342\n",
            "Collected article 343\n",
            "Collected article 344\n",
            "Collected article 345\n",
            "Collected article 346\n",
            "Collected article 347\n",
            "Collected article 348\n",
            "Collected article 349\n",
            "Collected article 350\n",
            "Collected article 351\n",
            "Collected article 352\n",
            "Collected article 353\n",
            "Collected article 354\n",
            "Collected article 355\n",
            "Collected article 356\n",
            "Collected article 357\n",
            "Collected article 358\n",
            "Collected article 359\n",
            "Collected article 360\n",
            "Collected article 361\n",
            "Collected article 362\n",
            "Collected article 363\n",
            "Collected article 364\n",
            "Collected article 365\n",
            "Collected article 366\n",
            "Collected article 367\n",
            "Collected article 368\n",
            "Collected article 369\n",
            "Collected article 370\n",
            "Collected article 371\n",
            "Collected article 372\n",
            "Collected article 373\n",
            "Collected article 374\n",
            "Collected article 375\n",
            "Collected article 376\n",
            "Collected article 377\n",
            "Collected article 378\n",
            "Collected article 379\n",
            "Collected article 380\n",
            "Collected article 381\n",
            "Collected article 382\n",
            "Collected article 383\n",
            "Collected article 384\n",
            "Collected article 385\n",
            "Collected article 386\n",
            "Collected article 387\n",
            "Collected article 388\n",
            "Collected article 389\n",
            "Collected article 390\n",
            "Collected article 391\n",
            "Collected article 392\n",
            "Collected article 393\n",
            "Collected article 394\n",
            "Collected article 395\n",
            "Collected article 396\n",
            "Collected article 397\n",
            "Collected article 398\n",
            "Collected article 399\n",
            "Collected article 400\n",
            "Collected article 401\n",
            "Collected article 402\n",
            "Collected article 403\n",
            "Collected article 404\n",
            "Collected article 405\n",
            "Collected article 406\n",
            "Collected article 407\n",
            "Collected article 408\n",
            "Collected article 409\n",
            "Collected article 410\n",
            "Collected article 411\n",
            "Collected article 412\n",
            "Collected article 413\n",
            "Collected article 414\n",
            "Collected article 415\n",
            "Collected article 416\n",
            "Collected article 417\n",
            "Collected article 418\n",
            "Collected article 419\n",
            "Collected article 420\n",
            "Collected article 421\n",
            "Collected article 422\n",
            "Collected article 423\n",
            "Collected article 424\n",
            "Collected article 425\n",
            "Collected article 426\n",
            "Collected article 427\n",
            "Collected article 428\n",
            "Collected article 429\n",
            "Collected article 430\n",
            "Collected article 431\n",
            "Collected article 432\n",
            "Collected article 433\n",
            "Collected article 434\n",
            "Collected article 435\n",
            "Collected article 436\n",
            "Collected article 437\n",
            "Collected article 438\n",
            "Collected article 439\n",
            "Collected article 440\n",
            "Collected article 441\n",
            "Collected article 442\n",
            "Collected article 443\n",
            "Collected article 444\n",
            "Collected article 445\n",
            "Collected article 446\n",
            "Collected article 447\n",
            "Collected article 448\n",
            "Collected article 449\n",
            "Collected article 450\n",
            "Collected article 451\n",
            "Collected article 452\n",
            "Collected article 453\n",
            "Collected article 454\n",
            "Collected article 455\n",
            "Collected article 456\n",
            "Collected article 457\n",
            "Collected article 458\n",
            "Collected article 459\n",
            "Collected article 460\n",
            "Collected article 461\n",
            "Collected article 462\n",
            "Collected article 463\n",
            "Collected article 464\n",
            "Collected article 465\n",
            "Collected article 466\n",
            "Collected article 467\n",
            "Collected article 468\n",
            "Collected article 469\n",
            "Collected article 470\n",
            "Collected article 471\n",
            "Collected article 472\n",
            "Collected article 473\n",
            "Collected article 474\n",
            "Collected article 475\n",
            "Collected article 476\n",
            "Collected article 477\n",
            "Collected article 478\n",
            "Collected article 479\n",
            "Collected article 480\n",
            "Collected article 481\n",
            "Collected article 482\n",
            "Collected article 483\n",
            "Collected article 484\n",
            "Collected article 485\n",
            "Collected article 486\n",
            "Collected article 487\n",
            "Collected article 488\n",
            "Collected article 489\n",
            "Collected article 490\n",
            "Collected article 491\n",
            "Collected article 492\n",
            "Collected article 493\n",
            "Collected article 494\n",
            "Collected article 495\n",
            "Collected article 496\n",
            "Collected article 497\n",
            "Collected article 498\n",
            "Collected article 499\n",
            "Collected article 500\n",
            "Collected article 501\n",
            "Collected article 502\n",
            "Collected article 503\n",
            "Collected article 504\n",
            "Collected article 505\n",
            "Collected article 506\n",
            "Collected article 507\n",
            "Collected article 508\n",
            "Collected article 509\n",
            "Collected article 510\n",
            "Collected article 511\n",
            "Collected article 512\n",
            "Collected article 513\n",
            "Collected article 514\n",
            "Collected article 515\n",
            "Collected article 516\n",
            "Collected article 517\n",
            "Collected article 518\n",
            "Collected article 519\n",
            "Collected article 520\n",
            "Collected article 521\n",
            "Collected article 522\n",
            "Collected article 523\n",
            "Collected article 524\n",
            "Collected article 525\n",
            "Collected article 526\n",
            "Collected article 527\n",
            "Collected article 528\n",
            "Collected article 529\n",
            "Collected article 530\n",
            "Collected article 531\n",
            "Collected article 532\n",
            "Collected article 533\n",
            "Collected article 534\n",
            "Collected article 535\n",
            "Collected article 536\n",
            "Collected article 537\n",
            "Collected article 538\n",
            "Collected article 539\n",
            "Collected article 540\n",
            "Collected article 541\n",
            "Collected article 542\n",
            "Collected article 543\n",
            "Collected article 544\n",
            "Collected article 545\n",
            "Collected article 546\n",
            "Collected article 547\n",
            "Collected article 548\n",
            "Collected article 549\n",
            "Collected article 550\n",
            "Collected article 551\n",
            "Collected article 552\n",
            "Collected article 553\n",
            "Collected article 554\n",
            "Collected article 555\n",
            "Collected article 556\n",
            "Collected article 557\n",
            "Collected article 558\n",
            "Collected article 559\n",
            "Collected article 560\n",
            "Collected article 561\n",
            "Collected article 562\n",
            "Collected article 563\n",
            "Collected article 564\n",
            "Collected article 565\n",
            "Collected article 566\n",
            "Collected article 567\n",
            "Collected article 568\n",
            "Collected article 569\n",
            "Collected article 570\n",
            "Collected article 571\n",
            "Collected article 572\n",
            "Collected article 573\n",
            "Collected article 574\n",
            "Collected article 575\n",
            "Collected article 576\n",
            "Collected article 577\n",
            "Collected article 578\n",
            "Collected article 579\n",
            "Collected article 580\n",
            "Collected article 581\n",
            "Collected article 582\n",
            "Collected article 583\n",
            "Collected article 584\n",
            "Collected article 585\n",
            "Collected article 586\n",
            "Collected article 587\n",
            "Collected article 588\n",
            "Collected article 589\n",
            "Collected article 590\n",
            "Collected article 591\n",
            "Collected article 592\n",
            "Collected article 593\n",
            "Collected article 594\n",
            "Collected article 595\n",
            "Collected article 596\n",
            "Collected article 597\n",
            "Collected article 598\n",
            "Collected article 599\n",
            "Collected article 600\n",
            "Collected article 601\n",
            "Collected article 602\n",
            "Collected article 603\n",
            "Collected article 604\n",
            "Collected article 605\n",
            "Collected article 606\n",
            "Collected article 607\n",
            "Collected article 608\n",
            "Collected article 609\n",
            "Collected article 610\n",
            "Collected article 611\n",
            "Collected article 612\n",
            "Collected article 613\n",
            "Collected article 614\n",
            "Collected article 615\n",
            "Collected article 616\n",
            "Collected article 617\n",
            "Collected article 618\n",
            "Collected article 619\n",
            "Collected article 620\n",
            "Collected article 621\n",
            "Collected article 622\n",
            "Collected article 623\n",
            "Collected article 624\n",
            "Collected article 625\n",
            "Collected article 626\n",
            "Collected article 627\n",
            "Collected article 628\n",
            "Collected article 629\n",
            "Collected article 630\n",
            "Collected article 631\n",
            "Collected article 632\n",
            "Collected article 633\n",
            "Collected article 634\n",
            "Collected article 635\n",
            "Collected article 636\n",
            "Collected article 637\n",
            "Collected article 638\n",
            "Collected article 639\n",
            "Collected article 640\n",
            "Collected article 641\n",
            "Collected article 642\n",
            "Collected article 643\n",
            "Collected article 644\n",
            "Collected article 645\n",
            "Collected article 646\n",
            "Collected article 647\n",
            "Collected article 648\n",
            "Collected article 649\n",
            "Collected article 650\n",
            "Collected article 651\n",
            "Collected article 652\n",
            "Collected article 653\n",
            "Collected article 654\n",
            "Collected article 655\n",
            "Collected article 656\n",
            "Collected article 657\n",
            "Collected article 658\n",
            "Collected article 659\n",
            "Collected article 660\n",
            "Collected article 661\n",
            "Collected article 662\n",
            "Collected article 663\n",
            "Collected article 664\n",
            "Collected article 665\n",
            "Collected article 666\n",
            "Collected article 667\n",
            "Collected article 668\n",
            "Collected article 669\n",
            "Collected article 670\n",
            "Collected article 671\n",
            "Collected article 672\n",
            "Collected article 673\n",
            "Collected article 674\n",
            "Collected article 675\n",
            "Collected article 676\n",
            "Collected article 677\n",
            "Collected article 678\n",
            "Collected article 679\n",
            "Collected article 680\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=36. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=37. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=38. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=39. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=40. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=41. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=42. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=43. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=44. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=45. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=46. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=47. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=48. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=49. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=50. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=51. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=52. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=53. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=54. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=55. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=56. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=57. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=58. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=59. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=60. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=61. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=62. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=63. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=64. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=65. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=66. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=67. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=68. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=69. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=70. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=71. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=72. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=73. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=74. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=75. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=76. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=77. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=78. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=79. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=80. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=81. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=82. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=83. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=84. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=85. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=86. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=87. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=88. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=89. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=90. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=91. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=92. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=93. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=94. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=95. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=96. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=97. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=98. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=99. Status code: 403\n",
            "Failed to fetch https://dl.acm.org/action/doSearch?AllField=XYZ&startPage=100. Status code: 403\n",
            "Number of articles collected: 680\n",
            "\n",
            "Article 1:\n",
            "{'title': 'Antibiotic Inventory Policy Design for Minimizing Total Inventory Costs in Pharmacies based on ABC-Fuzzy Classification Analysis Approach using Probabilistic Continuous Review Method', 'venue': 'No venue information available', 'year': 'No publication year available', 'authors': 'No author information available', 'abstract': 'No abstract available'}\n",
            "\n",
            "Article 2:\n",
            "{'title': 'In Deep Security Management Strategy: Vulnerability Assessment Within Educational Institution', 'venue': 'No venue information available', 'year': 'No publication year available', 'authors': 'No author information available', 'abstract': 'No abstract available'}\n",
            "\n",
            "Article 3:\n",
            "{'title': 'Trajectory computation in a problem-oriented language', 'venue': 'No venue information available', 'year': 'No publication year available', 'authors': 'No author information available', 'abstract': 'No abstract available'}\n",
            "\n",
            "Article 4:\n",
            "{'title': \"Security Vulnerability Analysis using Penetration Testing Execution Standard (PTES): Case Study of Government's Website\", 'venue': 'No venue information available', 'year': 'No publication year available', 'authors': 'No author information available', 'abstract': 'No abstract available'}\n",
            "\n",
            "Article 5:\n",
            "{'title': 'The Usage of Agile Adoption Framework to Assess Scrum Process and Recommend Improvements', 'venue': 'No venue information available', 'year': 'No publication year available', 'authors': 'No author information available', 'abstract': 'No abstract available'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I57NXsauCec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "ee923254-4973-4460-e765-7a5bd7306f95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n1. Download and install Octoparse from https://www.octoparse.com/.\\n\\n2. Open Octoparse: Launch the Octoparse software and create a new scraping task.\\n\\n3. Navigate to the Website: Enter the URL of the website we want to scrape (in this case, the Twitter profile page of VWGroup: https://twitter.com/VWGroup).\\n\\n4. Set up Pagination (if needed): If the data is spread across multiple pages (e.g., tweets on different pages), set up pagination to navigate through the pages automatically.\\n\\n5. Select the Data to Extract: Use Octoparse's point-and-click interface to select the data elements we want to extract. In this case, we'll select the title, image, tweet text, username, timestamp, and any other relevant information.\\n\\n6. Refine the Selection: After selecting the data elements, we can refine the selection to ensure accurate extraction. For example, we may need to handle dynamic elements, handle pagination, or deal with nested structures.\\n\\n7. Start the Extraction: Once we've configured the scraping task, start the extraction process. Octoparse will visit the website, scrape the data according to our configuration, and save it to a file.\\n\\n8. Review and Export the Data: After the extraction is complete, review the extracted data using Excel to ensure its accuracy.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# write your answer here\n",
        "\"\"\"\n",
        "1. Download and install Octoparse from https://www.octoparse.com/.\n",
        "\n",
        "2. Open Octoparse: Launch the Octoparse software and create a new scraping task.\n",
        "\n",
        "3. Navigate to the Website: Enter the URL of the website we want to scrape (in this case, the Twitter profile page of VWGroup: https://twitter.com/VWGroup).\n",
        "\n",
        "4. Set up Pagination (if needed): If the data is spread across multiple pages (e.g., tweets on different pages), set up pagination to navigate through the pages automatically.\n",
        "\n",
        "5. Select the Data to Extract: Use Octoparse's point-and-click interface to select the data elements we want to extract.\n",
        "In this case, we will select the title, image, tweet text, username, timestamp, and any other relevant information.\n",
        "\n",
        "6. Refine the Selection: After selecting the data elements, we can refine the selection to ensure accurate extraction.\n",
        "\n",
        "\n",
        "7. Start the Extraction: Once we've configured the scraping task, start the extraction process.\n",
        "Octoparse will visit the website, scrape the data according to our configuration, and save it to a file.\n",
        "\n",
        "8. Review and Export the Data: After the extraction is complete, review the extracted data using Excel to ensure its accuracy.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "7fc3d68e-35d1-4860-8dab-9126b1cbe940"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLearning Experience:\\n    Working on web scraping tasks provided an excellent learning experience in understanding the process of extracting data from online sources. The key concepts I found most beneficial were understanding HTML structure, CSS selectors, and the use of libraries like BeautifulSoup for parsing HTML content. Learning how to navigate through web pages programmatically and extract specific information was particularly valuable. Additionally, learning about handling pagination, dynamic content, and dealing with rate limiting or anti-scraping measures added depth to my understanding of web scraping techniques.\\n\\nChallenges Encountered:\\n    One of the challenges I encountered was dealing with websites that have complex HTML structures or dynamically loaded content. During such occasions exactly locating the suitable elements of the CSS selectors to work against the intended data elements becomes more difficult. Moreover, some sites can implement anti-scraping technologies, for instance, rate limiting or Captcha problems that make scraping this information harder. Faced with such obstacles as time out and IP-based blocking, I experimented with various CSS selectors, setTimeout window between requests and utilized proxy servers.\\n\\nRelevance to Your Field of Study:\\n    Accessing and processing the information from internet platforms can be directly related to academic disciplines ranging from mine to others that are used in my study. Being a researcher in the field of web scrapping gives me the advantage of exploiting the mass of data available online and conducting large scale analyses, tracking developments, and detecting many patterns. For instance, social media platforms data will be used to gather the opinions of the public on some topics and industry reports data could be analyzed to get the trends in the market. This provides me with the opportunity to draw conclusions and correlations based on rigorous, in-depth study and hence catalyzes the new knowledge in my field of expertise. Also, utilizing web scraping skills for data collection can be efficient speeding up research work and saving time and resources. The fact that the data availability from online sources as well as the detailed analytical information vastly adds to the depth and breadth of my research and works.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "Learning Experience:\n",
        "    Working on web scraping tasks provided an excellent learning experience in understanding the process of extracting data from online sources.\n",
        "    The key concepts I found most beneficial were understanding HTML structure, CSS selectors, and the use of libraries like BeautifulSoup for parsing HTML content. Learning how to navigate through web pages programmatically and extract specific information was particularly valuable. Additionally, learning about handling pagination, dynamic content,\n",
        "    and dealing with rate limiting or anti-scraping measures added depth to my understanding of web scraping techniques.\n",
        "\n",
        "Challenges Encountered:\n",
        "    One of the challenges I encountered was dealing with websites that have complex HTML structures or dynamically loaded content.\n",
        "    During such occasions exactly locating the suitable elements of the CSS selectors to work against the intended data elements becomes more difficult.\n",
        "    Moreover, some sites can implement anti-scraping technologies, for instance, rate limiting or Captcha problems that make scraping this information harder.\n",
        "    Faced with such obstacles as time out and IP-based blocking, I experimented with various CSS selectors, setTimeout window between requests and utilized proxy servers.\n",
        "\n",
        "Relevance to Your Field of Study:\n",
        "    Accessing and processing the information from internet platforms can be directly related to academic disciplines ranging from mine to others that are used in my study.\n",
        "    Being a researcher in the field of web scrapping gives me the advantage of exploiting the mass of data available online and conducting large scale analyses, tracking developments, and detecting many patterns.\n",
        "    For instance, social media platforms data will be used to gather the opinions of the public on some topics and industry reports data could be analyzed to get the trends in the market.\n",
        "    This provides me with the opportunity to draw conclusions and correlations based on rigorous, in-depth study and hence catalyzes the new knowledge in my field of expertise.\n",
        "    Also, utilizing web scraping skills for data collection can be efficient speeding up research work and saving time and resources.\n",
        "    The fact that the data availability from online sources as well as the detailed analytical information vastly adds to the depth and breadth of my research and works.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qf-NvwC2sqaI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}